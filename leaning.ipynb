{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU（Rectified Linear Unit）激活函数是深度学习中常用的一种激活函数。它的数学表达式非常简单：\n",
    "\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "这意味着，当输入 \\(x\\) 大于0时，输出就是 \\(x\\) 本身；当输入 \\(x\\) 小于或等于0时，输出为0。\n",
    "\n",
    "### **ReLU的优点**\n",
    "1. **计算简单**：ReLU的计算非常简单，只需要比较输入和0的大小，这使得它在计算上非常高效。\n",
    "2. **梯度消失问题较少**：相比于Sigmoid和Tanh等激活函数，ReLU在正区间的梯度恒为1，这有助于缓解梯度消失问题，从而加速神经网络的训练。\n",
    "\n",
    "### **ReLU的缺点**\n",
    "1. **Dying ReLU问题**：在训练过程中，某些神经元可能会因为输入总是小于0而导致输出恒为0，这些神经元就“死掉”了，不再对网络的学习有贡献。\n",
    "2. **不对称性**：ReLU对负值的处理方式可能会导致一些信息的丢失。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是梯度？\n",
    "\n",
    "在机器学习和深度学习中，**梯度**是一个向量，表示函数在某一点的方向导数。具体来说，梯度指的是损失函数相对于模型参数的导数。梯度的方向指向函数值增加最快的方向，而梯度的负方向则指向函数值减少最快的方向。\n",
    "\n",
    "在神经网络的训练过程中，梯度用于更新模型的参数，以最小化损失函数。这个过程通常通过反向传播算法（Backpropagation）和梯度下降优化算法来实现。\n",
    "\n",
    "### 什么是梯度消失？\n",
    "\n",
    "**梯度消失**是指在深度神经网络的训练过程中，梯度在反向传播时逐层变小，最终导致靠近输入层的梯度几乎消失。这会使得这些层的参数几乎无法更新，从而影响模型的训练效果。\n",
    "\n",
    "梯度消失问题通常发生在使用Sigmoid或Tanh等激活函数时，因为这些函数的导数在输入值较大或较小时会变得非常小。随着网络层数的增加，这些小梯度在反向传播过程中会逐层相乘，导致梯度迅速衰减。\n",
    "\n",
    "### 为什么将隐藏层的数据进行线性转换之后会得到潜在向量？\n",
    "\n",
    "在编码器（Encoder）结构中，隐藏层的数据通过线性转换得到潜在向量（latent vector），主要是为了将输入数据映射到一个低维的潜在空间。这种转换有以下几个目的：\n",
    "\n",
    "1. **特征提取**：通过线性转换和非线性激活函数，模型可以从输入数据中提取出更有代表性的特征。这些特征可以更好地表示输入数据的本质信息。\n",
    "\n",
    "2. **降维**：潜在向量通常是低维的，这有助于减少数据的维度，从而降低计算复杂度和存储需求。在自动编码器（Autoencoder）中，编码器的目标就是将高维输入数据压缩到低维的潜在空间。\n",
    "\n",
    "3. **数据表示**：潜在向量可以看作是输入数据的一种紧凑表示，它保留了输入数据的主要信息，同时去除了冗余信息。这种表示可以用于数据压缩、特征提取和生成任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义编码器（Encoder）\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # 输入层到隐藏层的全连接层\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)  # 隐藏层到潜在空间的全连接层\n",
    "        self.fc3 = nn.Linear(hidden_dim, latent_dim)  # 隐藏层到潜在空间的全连接层\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        h = torch.relu(self.fc1(x))  # 通过fc1线性层进行转换，得到隐藏层的输出,隐藏层的输出通过ReLU激活函数引入非线性\n",
    "        h = torch.relu(self.fc2(h))  # 隐藏层的输出通过fc2线性层转换\n",
    "        z = self.fc3(h)  # 得到连续的潜在向量，得到潜在向量z\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型的要素和定义\n",
    "\n",
    "一个神经网络模型通常包含以下几个要素：\n",
    "\n",
    "1. **层（Layers）**：模型的基本构建块，包括全连接层（`nn.Linear`）、卷积层（`nn.Conv2d`）、循环层（`nn.LSTM`）等。\n",
    "2. **激活函数（Activation Functions）**：引入非线性，如 ReLU、Sigmoid、Tanh 等。\n",
    "3. **前向传播（Forward Pass）**：定义数据如何通过网络进行传播。\n",
    "4. **损失函数（Loss Function）**：用于衡量模型预测与真实值之间的差距，如均方误差（MSE）、交叉熵损失（Cross-Entropy Loss）等。\n",
    "5. **优化器（Optimizer）**：用于更新模型参数以最小化损失函数，如 SGD、Adam 等。\n",
    "\n",
    "\n",
    "### 层与层之间是如何沟通的？\n",
    "\n",
    "在神经网络中，层与层之间的沟通是通过前向传播（forward propagation）实现的。具体来说，每一层的输出会作为下一层的输入。这个过程可以分为以下几个步骤：\n",
    "\n",
    "1. **输入数据传递到第一层**：输入数据首先传递到网络的第一层（通常是输入层）。\n",
    "2. **计算输出**：每一层根据其权重、偏置和激活函数计算输出。\n",
    "3. **传递到下一层**：当前层的输出作为下一层的输入，依次类推，直到最后一层（通常是输出层）。\n",
    "\n",
    "### 处理不同维度的层\n",
    "\n",
    "当两个层的维度不同（即输入和输出的特征数不同）时，通常通过以下几种方式来处理：\n",
    "\n",
    "1. **全连接层（Linear Layer）**：全连接层可以将输入的任意维度映射到输出的任意维度。它通过一个权重矩阵和一个偏置向量来实现这种映射。\n",
    "    ```python\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    # 定义一个全连接层，将输入维度10映射到输出维度5\n",
    "    fc = nn.Linear(10, 5)\n",
    "    input_tensor = torch.randn(3, 10)  # 输入张量，形状为 (batch_size, input_dim)\n",
    "    output_tensor = fc(input_tensor)  # 输出张量，形状为 (batch_size, output_dim)\n",
    "    print(output_tensor.shape)  # 输出: torch.Size([3, 5])\n",
    "    ```\n",
    "\n",
    "2. **卷积层（Convolutional Layer）**：卷积层通过卷积核（filter）来处理输入数据，可以改变数据的维度。\n",
    "    ```python\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    # 定义一个卷积层，将输入通道数3映射到输出通道数16\n",
    "    conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "    input_tensor = torch.randn(1, 3, 32, 32)  # 输入张量，形状为 (batch_size, in_channels, height, width)\n",
    "    output_tensor = conv(input_tensor)  # 输出张量，形状为 (batch_size, out_channels, height, width)\n",
    "    print(output_tensor.shape)  # 输出: torch.Size([1, 16, 32, 32])\n",
    "    ```\n",
    "\n",
    "3. **批量归一化层（Batch Normalization Layer）**：批量归一化层可以在不同维度之间进行归一化处理，通常用于加速训练和稳定模型。\n",
    "    ```python\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    # 定义一个批量归一化层\n",
    "    bn = nn.BatchNorm1d(10)\n",
    "    input_tensor = torch.randn(3, 10)  # 输入张量，形状为 (batch_size, num_features)\n",
    "    output_tensor = bn(input_tensor)  # 输出张量，形状为 (batch_size, num_features)\n",
    "    print(output_tensor.shape)  # 输出: torch.Size([3, 10])\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新版本的代码簿（Codebook）\n",
    "class Codebook(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(Codebook, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)  # 嵌入层\n",
    "        self.embeddings.weight.data.uniform_(-1.0 / num_embeddings, 1.0 / num_embeddings)  # 初始化码本的范围\n",
    "    \n",
    "    def forward(self, z):\n",
    "        # Flatten z to fit into the embedding\n",
    "        z_flattened = z.view(-1, self.embedding_dim)\n",
    "        \n",
    "        # Compute L2 distance between z and the embeddings\n",
    "        distances = (\n",
    "            torch.sum(z_flattened**2, dim=1, keepdim=True) \n",
    "            + torch.sum(self.embeddings.weight**2, dim=1)\n",
    "            - 2 * torch.matmul(z_flattened, self.embeddings.weight.t())\n",
    "        )\n",
    "        \n",
    "        # Encoding\n",
    "        min_encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        z_quantized = self.embeddings(min_encoding_indices).view(z.shape)\n",
    "        \n",
    "        # Use a straight-through estimator for the gradients\n",
    "        z_quantized = z + (z_quantized - z).detach()\n",
    "        \n",
    "        # Compute the loss for maintaining the codebook (commitment loss)\n",
    "        commitment_loss = F.mse_loss(z_quantized.detach(), z)\n",
    "        \n",
    "        return commitment_loss, z_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Sigmoid激活函数**：\n",
    "    - **定义**：Sigmoid激活函数的数学表达式为：\n",
    "      $$\n",
    "      \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "      $$\n",
    "      该函数将输入值映射到0和1之间。\n",
    "    - **优点**：Sigmoid函数能够将输出值限制在0到1之间，这在需要概率输出或归一化输出的场景中非常有用。\n",
    "    - **使用场景**：通常用于输出层，特别是在需要输出概率值的任务中，如二分类问题或重构图像像素值（像素值通常在0到1之间）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义解码器（Decoder）\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim)  # 潜在空间到隐藏层的全连接层\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)  # 增加一个隐藏层\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)  # 隐藏层到输出层的全连接层\n",
    "    \n",
    "    def forward(self, z_q):\n",
    "        h = torch.relu(self.fc1(z_q))  # 使用ReLU激活函数\n",
    "        h = torch.relu(self.fc2(h))  # 使用ReLU激活函数\n",
    "        x_recon = torch.sigmoid(self.fc3(h))  # 使用Sigmoid激活函数\n",
    "        return x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义VQ-VAE模型\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_embeddings):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)  # 编码器\n",
    "        self.codebook = Codebook(num_embeddings, latent_dim)  # 代码簿\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)  # 解码器\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)  # 编码输入数据\n",
    "        commitment_loss, z_q = self.codebook(z)  # 量化潜在向量\n",
    "        x_recon = self.decoder(z_q)  # 解码量化后的向量\n",
    "        return x_recon, z, z_q, commitment_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vqvae(model, data, num_epochs=10_0000, min_loss=0.02, learning_rate=1e-3, batch_size=4):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam优化器\n",
    "    criterion = nn.MSELoss()  # 均方误差损失函数\n",
    "    \n",
    "    # 将数据分成训练集和校验集\n",
    "    dataset = TensorDataset(data)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # 设定模型为训练模式\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            batch_data = batch[0].to(device)\n",
    "            optimizer.zero_grad()  # 清空梯度\n",
    "            x_recon, z, z_q, commitment_loss = model(batch_data)  # 前向传播\n",
    "            recon_loss = criterion(x_recon, batch_data)  # 计算重建损失\n",
    "            loss = recon_loss + commitment_loss  # 总损失\n",
    "            loss.backward()  # 反向传播\n",
    "            optimizer.step()  # 更新参数\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_epoch_loss:.4f}')\n",
    "        \n",
    "        # 校验模型\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                batch_data = batch[0].to(device)\n",
    "                x_recon, z, z_q, commitment_loss = model(batch_data)\n",
    "                loss = criterion(x_recon, batch_data).item()\n",
    "                val_loss += loss\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # 检查是否满足退出条件\n",
    "        if avg_val_loss <= min_loss:\n",
    "            print(f'训练提前终止于Epoch {epoch + 1}，因为校验Loss达到了{avg_val_loss:.4f}')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vqvae(model, data):\n",
    "    model.eval()  # 设定模型为评估模式\n",
    "    criterion = nn.MSELoss()  # 均方误差损失函数\n",
    "    total_loss = 0\n",
    "    max_loss = 0\n",
    "    all_indices = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(data)):\n",
    "            x = data[i].unsqueeze(0).to(device)  # 获取单个样本并增加批量维度\n",
    "            x_recon, z, z_q, indices = model(x)\n",
    "            loss = criterion(x_recon, x).item()\n",
    "            total_loss += loss\n",
    "            if loss > max_loss:\n",
    "                max_loss = loss\n",
    "            all_indices.append(indices.item())\n",
    "    \n",
    "    avg_loss = total_loss / len(data)\n",
    "    print(f'平均Loss: {avg_loss:.4f}')\n",
    "    print(f'最高Loss: {max_loss:.4f}')\n",
    "    print(f'代码向量的索引: {all_indices}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100000], Loss: 0.1165\n",
      "Validation Loss: 0.0936\n",
      "Epoch [2/100000], Loss: 0.0932\n",
      "Validation Loss: 0.0936\n",
      "Epoch [3/100000], Loss: 0.0848\n",
      "Validation Loss: 0.0935\n",
      "Epoch [4/100000], Loss: 0.0813\n",
      "Validation Loss: 0.0935\n",
      "Epoch [5/100000], Loss: 0.0802\n",
      "Validation Loss: 0.0936\n",
      "Epoch [6/100000], Loss: 0.0799\n",
      "Validation Loss: 0.0936\n",
      "Epoch [7/100000], Loss: 0.0797\n",
      "Validation Loss: 0.0936\n",
      "Epoch [8/100000], Loss: 0.0795\n",
      "Validation Loss: 0.0937\n",
      "Epoch [9/100000], Loss: 0.0794\n",
      "Validation Loss: 0.0936\n",
      "Epoch [10/100000], Loss: 0.0793\n",
      "Validation Loss: 0.0937\n",
      "Epoch [11/100000], Loss: 0.0792\n",
      "Validation Loss: 0.0937\n",
      "Epoch [12/100000], Loss: 0.0791\n",
      "Validation Loss: 0.0938\n",
      "Epoch [13/100000], Loss: 0.0792\n",
      "Validation Loss: 0.0937\n",
      "Epoch [14/100000], Loss: 0.0791\n",
      "Validation Loss: 0.0937\n",
      "Epoch [15/100000], Loss: 0.0791\n",
      "Validation Loss: 0.0939\n",
      "Epoch [16/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0938\n",
      "Epoch [17/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0939\n",
      "Epoch [18/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0939\n",
      "Epoch [19/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0937\n",
      "Epoch [20/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0937\n",
      "Epoch [21/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0938\n",
      "Epoch [22/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [23/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0937\n",
      "Epoch [24/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [25/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0939\n",
      "Epoch [26/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [27/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [28/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0937\n",
      "Epoch [29/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0936\n",
      "Epoch [30/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [31/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [32/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [33/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [34/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0938\n",
      "Epoch [35/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [36/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [37/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [38/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [39/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [40/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [41/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [42/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [43/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [44/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [45/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [46/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [47/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [48/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0937\n",
      "Epoch [49/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0938\n",
      "Epoch [50/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [51/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0939\n",
      "Epoch [52/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [53/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [54/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [55/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [56/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0937\n",
      "Epoch [57/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [58/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0936\n",
      "Epoch [59/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0936\n",
      "Epoch [60/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0938\n",
      "Epoch [61/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [62/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [63/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0938\n",
      "Epoch [64/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [65/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [66/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [67/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [68/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [69/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [70/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [71/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [72/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [73/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [74/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0938\n",
      "Epoch [75/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0937\n",
      "Epoch [76/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [77/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [78/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [79/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [80/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [81/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0938\n",
      "Epoch [82/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [83/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [84/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [85/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0936\n",
      "Epoch [86/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0938\n",
      "Epoch [87/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [88/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0936\n",
      "Epoch [89/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [90/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0938\n",
      "Epoch [91/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [92/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [93/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [94/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [95/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0937\n",
      "Epoch [96/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [97/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [98/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [99/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [100/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [101/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0936\n",
      "Epoch [102/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [103/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [104/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0939\n",
      "Epoch [105/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [106/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [107/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0937\n",
      "Epoch [108/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [109/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [110/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [111/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [112/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [113/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [114/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0936\n",
      "Epoch [115/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [116/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [117/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [118/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [119/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [120/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [121/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [122/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [123/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [124/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [125/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [126/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [127/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [128/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [129/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [130/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [131/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [132/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [133/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [134/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [135/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [136/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [137/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [138/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [139/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0937\n",
      "Epoch [140/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [141/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [142/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0938\n",
      "Epoch [143/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [144/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0938\n",
      "Epoch [145/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [146/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [147/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [148/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [149/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [150/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [151/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [152/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [153/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0937\n",
      "Epoch [154/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [155/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [156/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [157/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [158/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [159/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0938\n",
      "Epoch [160/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [161/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [162/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [163/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0939\n",
      "Epoch [164/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [165/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [166/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [167/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [168/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [169/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [170/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [171/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [172/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0938\n",
      "Epoch [173/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [174/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [175/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [176/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [177/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [178/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [179/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0936\n",
      "Epoch [180/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [181/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [182/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [183/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [184/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0937\n",
      "Epoch [185/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [186/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [187/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [188/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [189/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [190/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [191/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [192/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [193/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [194/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [195/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [196/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [197/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [198/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [199/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [200/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [201/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [202/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [203/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0936\n",
      "Epoch [204/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [205/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [206/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [207/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [208/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [209/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0938\n",
      "Epoch [210/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [211/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0936\n",
      "Epoch [212/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [213/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [214/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [215/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [216/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [217/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [218/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [219/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [220/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [221/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0936\n",
      "Epoch [222/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [223/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [224/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [225/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [226/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [227/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [228/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [229/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [230/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [231/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [232/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [233/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [234/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [235/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [236/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [237/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [238/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [239/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [240/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [241/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [242/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [243/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [244/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0937\n",
      "Epoch [245/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [246/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [247/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [248/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [249/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [250/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [251/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [252/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [253/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [254/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [255/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [256/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [257/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [258/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [259/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [260/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [261/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [262/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [263/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [264/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [265/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [266/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [267/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [268/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [269/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [270/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [271/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [272/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [273/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [274/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [275/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0937\n",
      "Epoch [276/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [277/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [278/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [279/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [280/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [281/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [282/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [283/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [284/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [285/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [286/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [287/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0938\n",
      "Epoch [288/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [289/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [290/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [291/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [292/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [293/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [294/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [295/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [296/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [297/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [298/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [299/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [300/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [301/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [302/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [303/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [304/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [305/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [306/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [307/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [308/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [309/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [310/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [311/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [312/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [313/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [314/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [315/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [316/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [317/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [318/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [319/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [320/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [321/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [322/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [323/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [324/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [325/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [326/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [327/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0936\n",
      "Epoch [328/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [329/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [330/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [331/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [332/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [333/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [334/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [335/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [336/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [337/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [338/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [339/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [340/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [341/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [342/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [343/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [344/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [345/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [346/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [347/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [348/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [349/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [350/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [351/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [352/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [353/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [354/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [355/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [356/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [357/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [358/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [359/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [360/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [361/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [362/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [363/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [364/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [365/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [366/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [367/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0936\n",
      "Epoch [368/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [369/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [370/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [371/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [372/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [373/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [374/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [375/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [376/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [377/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [378/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [379/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [380/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [381/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [382/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [383/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [384/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [385/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [386/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [387/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [388/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [389/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [390/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [391/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [392/100000], Loss: 0.0790\n",
      "Validation Loss: 0.0936\n",
      "Epoch [393/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [394/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [395/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [396/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [397/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [398/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [399/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [400/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [401/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [402/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [403/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [404/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [405/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [406/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [407/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [408/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [409/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [410/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [411/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [412/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [413/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [414/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [415/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [416/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [417/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [418/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [419/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [420/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [421/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [422/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [423/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [424/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [425/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [426/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [427/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [428/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [429/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [430/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [431/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [432/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [433/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [434/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [435/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [436/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [437/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [438/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [439/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [440/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [441/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [442/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [443/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [444/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [445/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [446/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [447/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [448/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [449/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [450/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [451/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [452/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [453/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [454/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [455/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [456/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [457/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [458/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [459/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [460/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [461/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [462/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [463/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [464/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [465/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [466/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [467/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [468/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [469/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [470/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [471/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [472/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [473/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [474/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0940\n",
      "Epoch [475/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [476/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [477/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [478/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [479/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [480/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [481/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [482/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [483/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [484/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [485/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [486/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [487/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [488/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [489/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [490/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [491/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [492/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [493/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [494/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [495/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [496/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [497/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [498/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [499/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [500/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [501/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [502/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [503/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [504/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [505/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [506/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [507/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [508/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [509/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [510/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [511/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [512/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [513/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [514/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [515/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [516/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [517/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [518/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [519/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [520/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [521/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [522/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [523/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [524/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [525/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [526/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [527/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [528/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [529/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [530/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [531/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [532/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [533/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [534/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [535/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [536/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [537/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [538/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0936\n",
      "Epoch [539/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [540/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [541/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [542/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [543/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [544/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [545/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [546/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [547/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [548/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [549/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [550/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [551/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [552/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [553/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [554/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [555/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [556/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [557/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [558/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [559/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [560/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [561/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [562/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [563/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [564/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [565/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [566/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [567/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [568/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [569/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [570/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [571/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0936\n",
      "Epoch [572/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [573/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [574/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [575/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [576/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [577/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [578/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [579/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [580/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [581/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [582/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [583/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [584/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [585/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [586/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [587/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [588/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [589/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [590/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [591/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [592/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [593/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [594/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [595/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [596/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [597/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [598/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0936\n",
      "Epoch [599/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [600/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [601/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [602/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [603/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [604/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [605/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [606/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [607/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [608/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [609/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [610/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [611/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [612/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [613/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [614/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [615/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [616/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [617/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [618/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [619/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [620/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [621/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [622/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [623/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [624/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [625/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [626/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [627/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [628/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [629/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [630/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [631/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [632/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [633/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [634/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [635/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [636/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [637/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [638/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [639/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [640/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [641/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [642/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [643/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [644/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [645/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [646/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [647/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [648/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [649/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [650/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [651/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [652/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [653/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [654/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [655/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [656/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [657/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [658/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [659/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [660/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [661/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [662/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [663/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [664/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [665/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [666/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [667/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [668/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [669/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [670/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [671/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [672/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [673/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [674/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [675/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [676/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [677/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [678/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [679/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [680/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [681/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [682/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [683/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [684/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [685/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [686/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [687/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [688/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [689/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [690/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [691/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [692/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [693/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [694/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [695/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [696/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [697/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [698/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [699/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [700/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [701/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [702/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [703/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [704/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [705/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [706/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [707/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [708/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [709/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [710/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [711/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [712/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [713/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [714/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [715/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [716/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [717/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [718/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [719/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [720/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [721/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [722/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [723/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [724/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [725/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [726/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [727/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [728/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [729/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [730/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [731/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [732/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [733/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [734/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [735/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [736/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [737/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [738/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [739/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [740/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [741/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [742/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [743/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [744/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [745/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [746/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [747/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [748/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [749/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [750/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [751/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [752/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [753/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [754/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [755/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [756/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [757/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [758/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [759/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [760/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [761/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [762/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [763/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [764/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [765/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [766/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [767/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [768/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [769/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [770/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [771/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [772/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [773/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [774/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [775/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [776/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [777/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [778/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [779/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [780/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [781/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [782/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [783/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [784/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [785/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [786/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [787/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [788/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [789/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [790/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [791/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [792/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [793/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [794/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [795/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [796/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [797/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [798/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [799/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [800/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [801/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [802/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [803/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [804/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [805/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [806/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [807/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [808/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [809/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [810/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [811/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [812/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [813/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [814/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [815/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [816/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [817/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [818/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [819/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [820/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [821/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [822/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [823/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [824/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [825/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [826/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [827/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [828/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [829/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [830/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [831/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [832/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [833/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [834/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [835/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [836/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [837/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [838/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [839/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [840/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [841/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [842/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [843/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [844/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [845/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [846/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [847/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [848/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [849/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [850/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [851/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [852/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [853/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [854/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [855/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [856/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [857/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [858/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [859/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [860/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [861/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [862/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [863/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [864/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [865/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [866/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [867/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [868/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [869/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [870/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [871/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0939\n",
      "Epoch [872/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [873/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [874/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [875/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [876/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [877/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [878/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [879/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [880/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [881/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [882/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [883/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [884/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [885/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [886/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [887/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [888/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [889/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [890/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [891/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [892/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [893/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [894/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [895/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [896/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [897/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [898/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [899/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [900/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [901/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [902/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [903/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [904/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [905/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [906/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [907/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [908/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [909/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [910/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [911/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [912/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [913/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [914/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [915/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [916/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [917/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [918/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [919/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [920/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [921/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [922/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [923/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [924/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [925/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [926/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [927/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [928/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [929/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [930/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [931/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [932/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [933/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [934/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [935/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [936/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [937/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [938/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [939/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [940/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [941/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [942/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [943/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [944/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [945/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [946/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [947/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [948/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [949/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [950/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [951/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [952/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [953/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [954/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [955/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [956/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [957/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [958/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [959/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [960/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [961/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [962/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [963/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [964/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [965/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [966/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [967/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [968/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0939\n",
      "Epoch [969/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [970/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0937\n",
      "Epoch [971/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0936\n",
      "Epoch [972/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [973/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [974/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [975/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [976/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [977/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [978/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [979/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [980/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [981/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [982/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [983/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n",
      "Epoch [984/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [985/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0937\n",
      "Epoch [986/100000], Loss: 0.0788\n",
      "Validation Loss: 0.0938\n",
      "Epoch [987/100000], Loss: 0.0789\n",
      "Validation Loss: 0.0938\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m vqvae \u001b[38;5;241m=\u001b[39m VQVAE(input_dim, hidden_dim, latent_dim, num_embeddings)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 训练VQ-VAE模型\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrain_vqvae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvqvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 测试VQ-VAE模型\u001b[39;00m\n\u001b[1;32m     15\u001b[0m test_vqvae(vqvae, data)\n",
      "Cell \u001b[0;32mIn[14], line 19\u001b[0m, in \u001b[0;36mtrain_vqvae\u001b[0;34m(model, data, num_epochs, min_loss, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# 设定模型为训练模式\u001b[39;00m\n\u001b[1;32m     17\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m     20\u001b[0m     batch_data \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# 清空梯度\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:264\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 生成随机数据\n",
    "input_dim = 8\n",
    "hidden_dim = 16\n",
    "latent_dim = 8\n",
    "num_embeddings = 16\n",
    "data = torch.rand((100, input_dim)).to(device)\n",
    "\n",
    "# 初始化VQ-VAE模型\n",
    "vqvae = VQVAE(input_dim, hidden_dim, latent_dim, num_embeddings).to(device)\n",
    "\n",
    "# 训练VQ-VAE模型\n",
    "train_vqvae(vqvae, data)\n",
    "\n",
    "# 测试VQ-VAE模型\n",
    "test_vqvae(vqvae, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
