import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import logging
from tqdm import tqdm
from einops import rearrange
from transformers.cache_utils import Cache

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.utils.parametrize as P
from torch.nn.utils.parametrizations import weight_norm
from transformers import LlamaModel, LlamaConfig
    
    
class LlamaMLP(nn.Module):
    def __init__(self, hidden_size, intermediate_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = F.silu

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj
    
    
class GPT_warpper(nn.Module):
    def __init__(
        self, 
        gpt_config, 
        num_audio_tokens,
        num_text_tokens,
        num_vq=4,
        **kwargs,
        ):
        super().__init__()

        self.logger = logging.getLogger(__name__)
        self.gpt = self.build_model(gpt_config)
        self.model_dim = self.gpt.config.hidden_size 

        # ä¿å­˜éŸ³é¢‘åµŒå…¥å±‚çš„æ•°é‡ 
        self.num_vq = num_vq
        # åˆ›å»ºä¸€ä¸ªåŒ…å« num_vq ä¸ªåµŒå…¥å±‚çš„æ¨¡å—åˆ—è¡¨ï¼Œæ¯ä¸ªåµŒå…¥å±‚å°†**éŸ³é¢‘**ä»¤ç‰Œæ˜ å°„åˆ°æ¨¡å‹çš„éšè—å±‚ç»´åº¦ã€‚
        self.emb_code = nn.ModuleList([nn.Embedding(num_audio_tokens, self.model_dim) for i in range(self.num_vq)])
        # åˆ›å»ºä¸€ä¸ªåµŒå…¥å±‚ï¼Œå°†**æ–‡æœ¬**ä»¤ç‰Œæ˜ å°„åˆ°æ¨¡å‹çš„éšè—å±‚ç»´åº¦ã€‚
        self.emb_text = nn.Embedding(num_text_tokens, self.model_dim)
        
        # åˆ›å»ºä¸€ä¸ªçº¿æ€§å±‚ï¼Œç”¨äºå°†æ¨¡å‹çš„è¾“å‡ºæ˜ å°„å›æ–‡æœ¬ä»¤ç‰Œç©ºé—´ï¼Œå¹¶åº”ç”¨æƒé‡å½’ä¸€åŒ–ã€‚
        self.head_text = weight_norm(nn.Linear(self.model_dim, num_text_tokens, bias=False), name='weight')
        # åˆ›å»ºä¸€ä¸ªåŒ…å« num_vq ä¸ªçº¿æ€§å±‚çš„æ¨¡å—åˆ—è¡¨ï¼Œæ¯ä¸ªçº¿æ€§å±‚ç”¨äºå°†æ¨¡å‹çš„è¾“å‡ºæ˜ å°„å›éŸ³é¢‘ä»¤ç‰Œç©ºé—´ï¼Œå¹¶åº”ç”¨æƒé‡å½’ä¸€åŒ–ã€‚
        self.head_code = nn.ModuleList([weight_norm(nn.Linear(self.model_dim, num_audio_tokens, bias=False), name='weight') for i in range(self.num_vq)])

    def build_model(self, config):
        
        configuration = LlamaConfig(**config)
        model = LlamaModel(configuration)
        del model.embed_tokens
        
        return model
    
    def get_emb(self, input_ids, text_mask, **kwargs):

        emb_text = self.emb_text(input_ids[text_mask][:, 0])
        
        emb_code = [self.emb_code[i](input_ids[~text_mask][:, i]) for i in range(self.num_vq)]
        emb_code = torch.stack(emb_code, 2).sum(2)
        
        emb = torch.zeros((input_ids.shape[:-1])+(emb_text.shape[-1],), device=emb_text.device, dtype=emb_text.dtype)
        emb[text_mask] = emb_text
        emb[~text_mask] = emb_code.to(emb.dtype)
        
        return emb
    
    def prepare_inputs_for_generation(
        self, input_ids, # è¾“å…¥åºåˆ—çš„ IDã€‚
        past_key_values=None, # è¿‡å»çš„é”®å€¼å¯¹ï¼Œç”¨äºç¼“å­˜å…ˆå‰è®¡ç®—çš„æ³¨æ„åŠ›æƒé‡ã€‚
        attention_mask=None, # æ³¨æ„åŠ›æ©ç ï¼Œç”¨äºæŒ‡ç¤ºæ¨¡å‹åº”è¯¥å…³æ³¨å“ªäº›ä½ç½®ã€‚
        inputs_embeds=None, # è¾“å…¥çš„åµŒå…¥è¡¨ç¤ºã€‚
        cache_position=None, #ç¼“å­˜ä½ç½®ã€‚
        **kwargs
    ):
        """è¿™ä¸ªå‡½æ•°çš„ä¸»è¦ç›®çš„æ˜¯ä¸ºç”Ÿæˆè¿‡ç¨‹å‡†å¤‡è¾“å…¥æ•°æ®ã€‚å®ƒå¤„ç†è¾“å…¥åºåˆ—çš„ IDã€è¿‡å»çš„é”®å€¼å¯¹ã€æ³¨æ„åŠ›æ©ç ã€è¾“å…¥åµŒå…¥è¡¨ç¤ºå’Œç¼“å­˜ä½ç½®ç­‰ä¿¡æ¯ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­èƒ½å¤Ÿé«˜æ•ˆåœ°å¤„ç†è¾“å…¥æ•°æ®
        """
        # With static cache, the `past_key_values` is None
        # TODO joao: standardize interface for the different Cache classes and remove of this if
        has_static_cache = False
        # åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œpast_key_values å¯èƒ½ä¸º Noneï¼Œè¿™é€šå¸¸å‘ç”Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹çš„ç¬¬ä¸€æ­¥ã€‚
        # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹éœ€è¦ä»è‡ªæ³¨æ„åŠ›å±‚ä¸­è·å–ç¼“å­˜çš„é”®å€¼å¯¹ï¼Œä»¥ä¾¿åœ¨åç»­æ­¥éª¤ä¸­ä½¿ç”¨ã€‚
        # é€šè¿‡ç¼“å­˜å…ˆå‰è®¡ç®—çš„æ³¨æ„åŠ›é”®å€¼å¯¹ï¼Œæ¨¡å‹å¯ä»¥é¿å…é‡å¤è®¡ç®—ï¼Œä»è€Œæ˜¾è‘—æé«˜ç”Ÿæˆæ•ˆç‡ã€‚
        if past_key_values is None:
            # å°è¯•ä»æ¨¡å‹çš„è‡ªæ³¨æ„åŠ›å±‚ä¸­è·å–ç¼“å­˜çš„é”®å€¼å¯¹
            # è§£é‡Šï¼šåœ¨ GPT è¿™æ ·çš„è‡ªå›å½’æ¨¡å‹ï¼Œæ¯ä¸€æ­¥ç”Ÿæˆæ–°çš„ token æ—¶ï¼Œæ¨¡å‹éœ€è¦é‡æ–°è®¡ç®—æ‰€æœ‰å…ˆå‰ç”Ÿæˆçš„ token çš„æ³¨æ„åŠ›æƒé‡ã€‚
            # è¿™ä¼šå¯¼è‡´è®¡ç®—é‡éšç€ç”Ÿæˆé•¿åº¦çš„å¢åŠ è€Œçº¿æ€§å¢é•¿ã€‚
            # ä¸ºäº†ä¼˜åŒ–è¿™ä¸€è¿‡ç¨‹ï¼Œæ¨¡å‹ä¼šç¼“å­˜å…ˆå‰è®¡ç®—çš„æ³¨æ„åŠ›é”®å€¼å¯¹ï¼Œè¿™æ ·åœ¨ç”Ÿæˆæ–°çš„ token æ—¶ï¼Œåªéœ€è¦è®¡ç®—æ–°å¢çš„éƒ¨åˆ†ï¼Œè€Œä¸å¿…é‡å¤è®¡ç®—æ‰€æœ‰å…ˆå‰çš„ tokenã€‚
            # 
            past_key_values = getattr(self.gpt.layers[0].self_attn, "past_key_value", None)
            has_static_cache = past_key_values is not None

        past_length = 0
        if past_key_values is not None:
            if isinstance(past_key_values, Cache):
                # å¦‚æœcache_positionä¸ºç©ºåˆ™è°ƒç”¨ past_key_values.get_seq_length() è·å–ç¼“å­˜çš„åºåˆ—é•¿åº¦ã€‚
                past_length = cache_position[0] if cache_position is not None else past_key_values.get_seq_length()
                # ç¡®å®šç¼“å­˜çš„æœ€å¤§é•¿åº¦ï¼Œä»¥ä¾¿åœ¨åç»­è®¡ç®— cache_length æ—¶ä½¿ç”¨
                max_cache_length = (
                    torch.tensor(past_key_values.get_max_length(), device=input_ids.device)
                    if past_key_values.get_max_length() is not None
                    else None
                )
                # 
                cache_length = past_length if max_cache_length is None else torch.min(max_cache_length, past_length)
            # TODO joao: remove this `else` after `generate` prioritizes `Cache` objects
            else:
                # å¦‚æœ past_key_values ä¸æ˜¯ Cache å¯¹è±¡ï¼Œåˆ™å‡è®¾å®ƒæ˜¯ä¸€ä¸ªåŒ…å«é”®å€¼å¯¹çš„åˆ—è¡¨
                # é€šè¿‡ past_key_values[0][0].shape[2] è·å–ç¼“å­˜çš„åºåˆ—é•¿åº¦ï¼Œå¹¶å°†å…¶èµ‹å€¼ç»™ past_length å’Œ cache_length
                cache_length = past_length = past_key_values[0][0].shape[2]
                max_cache_length = None

            # Keep only the unprocessed tokens:
            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where
            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as
            # input)
            # å¦‚æœ attention_mask ä¸ä¸ºç©ºä¸”å…¶é•¿åº¦å¤§äº input_ids çš„é•¿åº¦ï¼Œåˆ™è¡¨ç¤ºæœ‰äº›è¾“å…¥æ˜¯ä½œä¸ºç¼“å­˜çš„ä¸€éƒ¨åˆ†ä¼ é€’çš„ï¼ˆä¾‹å¦‚ï¼Œå½“ä¼ é€’ input_embeds ä½œä¸ºè¾“å…¥æ—¶ï¼‰
            if attention_mask is not None and attention_mask.shape[1] > input_ids.shape[1]:
                # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ›´æ–° input_ids ä»¥ä»…ä¿ç•™æœªå¤„ç†çš„ tokenã€‚å…·ä½“æ¥è¯´ï¼Œä¿ç•™ä» input_ids çš„æœ«å°¾å¼€å§‹ï¼Œé•¿åº¦ä¸º attention_mask.shape[1] - past_length çš„éƒ¨åˆ†
                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]
            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard
            # input_ids based on the past_length.
            # å¦‚æœ past_length å°äº input_ids çš„é•¿åº¦ï¼Œåˆ™è¡¨ç¤º input_ids åŒ…å«æ‰€æœ‰è¾“å…¥ token
            elif past_length < input_ids.shape[1]:
                # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ›´æ–° input_ids ä»¥ä»…ä¿ç•™æœªå¤„ç†çš„ tokenã€‚å…·ä½“æ¥è¯´ï¼Œä¿ç•™ä» input_ids çš„ past_length ä½ç½®å¼€å§‹çš„éƒ¨åˆ†
                input_ids = input_ids[:, past_length:]
            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.

            # å¦‚æœ past_length å¤§äºæˆ–ç­‰äº input_ids çš„é•¿åº¦ï¼Œåˆ™å‡è®¾ input_ids ä»…åŒ…å«æœªå¤„ç†çš„ tokenã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸éœ€è¦å¯¹ input_ids è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚
            # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.
            # å¦‚æœ max_cache_length ä¸ä¸ºç©ºï¼Œä¸” attention_mask ä¸ä¸ºç©ºï¼Œå¹¶ä¸” cache_length åŠ ä¸Š input_ids çš„é•¿åº¦è¶…è¿‡ max_cache_lengthï¼Œåˆ™éœ€è¦è£å‰ª attention_mask
            if (
                max_cache_length is not None
                and attention_mask is not None
                and cache_length + input_ids.shape[1] > max_cache_length
            ):
                # æ›´æ–° attention_mask ä»¥ä»…ä¿ç•™æœ€å max_cache_length é•¿åº¦çš„éƒ¨åˆ†ã€‚
                # è¿™ç¡®ä¿äº† attention_mask çš„é•¿åº¦ä¸ä¼šè¶…è¿‡æœ€å¤§ç¼“å­˜é•¿åº¦
                attention_mask = attention_mask[:, -max_cache_length:]

        position_ids = kwargs.get("position_ids", None)
        if attention_mask is not None and position_ids is None:
            # create position_ids on the fly for batch generation
            position_ids = attention_mask.long().cumsum(-1) - 1
            position_ids.masked_fill_(attention_mask == 0, 1)
            if past_key_values:
                position_ids = position_ids[:, -input_ids.shape[1] :]

        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step
        if inputs_embeds is not None and past_key_values is None:
            # å¦‚æœ inputs_embeds ä¸ä¸ºç©ºä¸” past_key_values ä¸ºç©ºï¼Œåˆ™è¡¨ç¤ºè¿™æ˜¯ç”Ÿæˆè¿‡ç¨‹çš„ç¬¬ä¸€æ­¥ï¼Œæ­¤æ—¶åªä½¿ç”¨ inputs_embeds
            model_inputs = {"inputs_embeds": inputs_embeds}
        else:
            # The `contiguous()` here is necessary to have a static stride during decoding. torchdynamo otherwise
            # recompiles graphs as the stride of the inputs is a guard. Ref: https://github.com/huggingface/transformers/pull/29114
            # TODO: use `next_tokens` directly instead.
            # å¦åˆ™ï¼Œä½¿ç”¨ input_ids å¹¶è°ƒç”¨ contiguous() æ–¹æ³•ç¡®ä¿å…¶åœ¨è§£ç è¿‡ç¨‹ä¸­å…·æœ‰é™æ€æ­¥å¹…ã€‚è¿™æ˜¯ä¸ºäº†é¿å… torchdynamo é‡æ–°ç¼–è¯‘å›¾å½¢
            model_inputs = {"input_ids": input_ids.contiguous()}

        # ç¡®å®š input_lengthï¼Œå³è¾“å…¥åºåˆ—çš„é•¿åº¦ã€‚
        # åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ¨¡å‹å¯èƒ½ä¼šä½¿ç”¨ position_ids æ¥æ˜ç¡®æŒ‡å®šæ¯ä¸ª token çš„ä½ç½®
        # åœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œæ¨¡å‹å¯èƒ½åªä½¿ç”¨ input_idsï¼Œè€Œä¸éœ€è¦æ˜¾å¼çš„ position_ids
        input_length = position_ids.shape[-1] if position_ids is not None else input_ids.shape[-1]
        if cache_position is None:
            cache_position = torch.arange(past_length, past_length + input_length, device=input_ids.device)
        else:
            cache_position = cache_position[-input_length:]

        if has_static_cache:
            past_key_values = None

        model_inputs.update(
            {
                "position_ids": position_ids,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "use_cache": kwargs.get("use_cache"),
                "attention_mask": attention_mask,
            }
        )
        return model_inputs
    
    def generate(
        self, 
        emb,    # åˆå§‹çš„åµŒå…¥è¡¨ç¤º
        inputs_ids,     # è¾“å…¥çš„ token IDs
        temperature,    #  æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§
        eos_token,      # ç»“æŸ token çš„ ID
        attention_mask = None, # æ³¨æ„åŠ›æ©ç 
        max_new_token = 2048,  # æœ€å¤§ç”Ÿæˆ token æ•°é‡
        min_new_token = 0,     # æœ€å°ç”Ÿæˆ token æ•°é‡
        LogitsWarpers = [],    #  ç”¨äºä¿®æ”¹ logits çš„å‡½æ•°åˆ—è¡¨
        LogitsProcessors = [], # ç”¨äºå¤„ç† logits çš„å‡½æ•°åˆ—è¡¨
        infer_text=False,      # æ˜¯å¦ç”Ÿæˆæ–‡æœ¬
        return_attn=False,     # æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
        return_hidden=False,   # æ˜¯å¦è¿”å›éšè—çŠ¶æ€
    ):
        
        with torch.no_grad():   
        
            attentions = [] # æ³¨æ„åŠ›æƒé‡
            hiddens = []   # éšè—çŠ¶æ€
            
            # ç”¨äºè®°å½•è¾“å…¥åºåˆ—çš„åˆå§‹é•¿åº¦ã€‚åœ¨ç”Ÿæˆæ–° token æ—¶ï¼Œè¿™ä¸ªå€¼å¯ä»¥å¸®åŠ©ç¡®å®šæ–°ç”Ÿæˆçš„ token åº”è¯¥æ·»åŠ åˆ°è¾“å…¥åºåˆ—çš„å“ªä¸ªä½ç½®ã€‚
            # inputs_ids çš„ç¬¬äºŒä¸ªç»´åº¦çš„å¤§å°ï¼Œå³è¾“å…¥åºåˆ—çš„é•¿åº¦ã€‚inputs_ids æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º (batch_size, sequence_length, num_vq) çš„å¼ é‡ï¼Œå› æ­¤ start_idx ä¿å­˜äº†è¾“å…¥åºåˆ—çš„ä½ç½®
            start_idx = inputs_ids.shape[1]
            # è¿™è¡Œä»£ç åˆ›å»ºäº†ä¸€ä¸ªå½¢çŠ¶ä¸º (batch_size,) çš„å…¨é›¶å¼ é‡ï¼Œæ•°æ®ç±»å‹ä¸º torch.longï¼Œå¹¶ä¸”ä¸ inputs_ids ä½äºåŒä¸€ä¸ªè®¾å¤‡ä¸Šï¼ˆä¾‹å¦‚ CPU æˆ– GPUï¼‰
            # end_idx ç”¨äºè®°å½•æ¯ä¸ªæ ·æœ¬çš„ç»“æŸä½ç½®ã€‚åˆå§‹åŒ–ä¸ºå…¨é›¶æ„å‘³ç€åœ¨ç”Ÿæˆå¼€å§‹æ—¶ï¼Œæ‰€æœ‰æ ·æœ¬çš„ç»“æŸä½ç½®éƒ½æœªç¡®å®š
            end_idx = torch.zeros(inputs_ids.shape[0], device=inputs_ids.device, dtype=torch.long)
            #  åˆ›å»ºä¸€ä¸ªå½¢çŠ¶ä¸º(batch_size,) çš„å¸ƒå°”å¼ é‡ï¼Œå…¶ä¸­batch_size å°±æ˜¯è¾“å…¥åºåˆ—çš„æ‰¹æ¬¡å¤§å°(è¯´ç™½äº†å°±æ˜¯å¤šå°‘å¥è¯)ã€‚åˆå§‹åŒ–ä¸ºå…¨Falseï¼Œè¡¨ç¤ºæ‰€æœ‰æ ·æœ¬å°šæœªå®Œæˆç”Ÿæˆ
            finish = torch.zeros(inputs_ids.shape[0], device=inputs_ids.device).bool()
            
            # è¿™ä¸€æ­¥é€šè¿‡åœ¨ temperature å¼ é‡çš„ç¬¬ä¸€ä¸ªç»´åº¦æ·»åŠ ä¸€ä¸ªæ–°çš„ç»´åº¦ï¼Œå°† temperature ä»å½¢çŠ¶ (n,) å˜ä¸º (1, n)
            temperature = temperature[None]
            # è¿™ä¸€æ­¥å°† temperature å¼ é‡æ‰©å±•åˆ°å½¢çŠ¶ (batch_size, n)ï¼Œå…¶ä¸­ batch_size æ˜¯ inputs_ids çš„ç¬¬ä¸€ä¸ªç»´åº¦çš„å¤§å°ã€‚-1 è¡¨ç¤ºä¿æŒè¯¥ç»´åº¦çš„å¤§å°ä¸å˜
            temperature = temperature.expand(inputs_ids.shape[0], -1)
            # "b n -> (b n) 1"ï¼šè¿™ä¸€æ­¥å°† temperature å¼ é‡ä»å½¢çŠ¶ (batch_size, n) é‡æ’ä¸ºå½¢çŠ¶ ((batch_size * n), 1)ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒå°†ç¬¬ä¸€ä¸ªç»´åº¦ batch_size å’Œç¬¬äºŒä¸ªç»´åº¦ n åˆå¹¶æˆä¸€ä¸ªç»´åº¦ï¼Œå¹¶åœ¨æœ€åæ·»åŠ ä¸€ä¸ªæ–°çš„ç»´åº¦ã€‚
            temperature = rearrange(temperature, "b n -> (b n) 1")

            # ç”¨äºå­˜å‚¨æ³¨æ„åŠ›æ©ç 
            attention_mask_cache = torch.ones(  # åˆ›å»ºä¸€ä¸ªå…¨ä¸º 1 çš„å¼ é‡
                (inputs_ids.shape[0], # è¡¨ç¤ºæ‰¹æ¬¡å¤§å°ï¼ˆbatch sizeï¼‰ã€‚
                 inputs_ids.shape[1]+max_new_token,),  # è¡¨ç¤ºè¾“å…¥åºåˆ—çš„é•¿åº¦åŠ ä¸Šæœ€å¤§æ–°ç”Ÿæˆçš„ token æ•°ï¼Œä»¥ä¾¿å®¹çº³è¾“å…¥åºåˆ—å’Œç”Ÿæˆçš„token
                dtype=torch.bool, 
                device=inputs_ids.device)
            if attention_mask is not None:
                # åœ¨ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œæ³¨æ„åŠ›æ©ç ç”¨äºæŒ‡ç¤ºæ¨¡å‹åœ¨å¤„ç†è¾“å…¥åºåˆ—æ—¶åº”è¯¥å…³æ³¨å“ªäº›ä½ç½®ã€‚
                # attention_mask é€šå¸¸æ˜¯ä¸€ä¸ªå¸ƒå°”å‹å¼ é‡ï¼Œå…¶ä¸­å€¼ä¸º 1 çš„ä½ç½®è¡¨ç¤ºæ¨¡å‹åº”è¯¥å…³æ³¨çš„ tokenï¼Œå€¼ä¸º 0 çš„ä½ç½®è¡¨ç¤ºæ¨¡å‹åº”è¯¥å¿½ç•¥çš„ token
                # attention_mask.shape[1] è¡¨ç¤º attention_mask çš„åˆ—æ•°ï¼Œå³è¾“å…¥åºåˆ—çš„é•¿åº¦
                # é€šè¿‡è¿™ä¸€æ­¥æ“ä½œï¼Œattention_mask_cache çš„å‰ attention_mask.shape[1] åˆ—è¢«åˆå§‹åŒ–ä¸º attention_mask çš„å€¼ã€‚
                # è¿™ç¡®ä¿äº†åœ¨ç”Ÿæˆæ–° token ä¹‹å‰ï¼Œattention_mask_cache å·²ç»åŒ…å«äº†**è¾“å…¥åºåˆ—**çš„æ³¨æ„åŠ›æ©ç ä¿¡æ¯
                # è¿™æ ·å°±å¯ä»¥ç¡®ä¿æˆ‘ä»¬é¢„å…ˆè¾“å…¥æ–‡æœ¬çš„ä¸ºæœ‰æ•ˆåºåˆ—ï¼Œé¿å…å¼•å…¥æ— æ•ˆçš„æ³¨æ„åŠ›ä¿¡æ¯
                attention_mask_cache[:, :attention_mask.shape[1]] = attention_mask
            
            for i in tqdm(range(max_new_token)):
                # åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ¯æ¬¡è¿­ä»£éƒ½ä¼šè°ƒç”¨è¿™æ®µä»£ç ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨æ¯ä¸€æ­¥éƒ½èƒ½æ­£ç¡®å¤„ç†è¾“å…¥æ•°æ®ã€‚
                # é€šè¿‡ä¼ é€’ past_key_values å’Œæ›´æ–°çš„æ³¨æ„åŠ›æ©ç ï¼Œæ¨¡å‹å¯ä»¥é«˜æ•ˆåœ°ç”Ÿæˆæ–°çš„ tokenï¼Œè€Œä¸å¿…é‡å¤è®¡ç®—æ‰€æœ‰å…ˆå‰çš„ token çš„æ³¨æ„åŠ›æƒé‡ã€‚
                model_input = self.prepare_inputs_for_generation(
                    inputs_ids,     # è¿™æ˜¯è¾“å…¥çš„ token IDsã€‚å®ƒè¡¨ç¤ºå½“å‰ç”Ÿæˆåºåˆ—çš„ token ID åˆ—è¡¨ã€‚
                    outputs.past_key_values if i!=0 else None, # å¦‚æœä¸æ˜¯ç¬¬ä¸€æ¬¡è¿­ä»£åˆ™è·å¾—åœ¨å‰ä¸€æ¬¡ç”Ÿæˆè¿‡ç¨‹ä¸­è®¡ç®—å¹¶ç¼“å­˜çš„æ³¨æ„åŠ›é”®å€¼å¯¹
                    attention_mask_cache[:, :inputs_ids.shape[1]], # ä» attention_mask_cache ä¸­æˆªå–å‰ inputs_ids.shape[1] åˆ—ï¼Œç¡®ä¿æ³¨æ„åŠ›æ©ç çš„é•¿åº¦ä¸å½“å‰è¾“å…¥åºåˆ—çš„é•¿åº¦ä¸€è‡´
                    use_cache=True)
            
                if i == 0:
                    # åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£æ—¶ï¼Œä½¿ç”¨åˆå§‹åµŒå…¥ `emb`ï¼Œ
                    model_input['inputs_embeds'] = emb
                else:
                    # ä¹‹åæ ¹æ® `infer_text` å†³å®šä½¿ç”¨æ–‡æœ¬åµŒå…¥è¿˜æ˜¯éŸ³é¢‘åµŒå…¥ã€‚
                    if infer_text:
                        model_input['inputs_embeds'] = self.emb_text(model_input['input_ids'][:,:,0])
                    else:
                        # code_emb = [self.emb_code[i](model_input['input_ids'][:,:,i]) for i in range(self.num_vq)]
                        # model_input['inputs_embeds'] = torch.stack(code_emb, 3).sum(3)
                        
                        code_emb = []   # ç”¨äºå­˜å‚¨æ¯ä¸ªå‘é‡é‡åŒ–å±‚çš„åµŒå…¥è¡¨ç¤º
                        # é€šè¿‡éå†å¤šä¸ªå‘é‡é‡åŒ–å±‚ï¼ˆVQ å±‚ï¼‰ï¼Œä¸ºæ¯ä¸ªå±‚çš„ token IDs ç”ŸæˆåµŒå…¥è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•å¯ä»¥æ•æ‰è¾“å…¥åºåˆ—ä¸­ä¸åŒå±‚æ¬¡çš„ä¿¡æ¯
                        for i in range(self.num_vq):
                            # model_input['input_ids'] çš„å½¢çŠ¶ä¸º (batch_size, sequence_length, num_vq)
                            input_ids_i = model_input['input_ids'][:, :, i] # è·å–ç¬¬ i ä¸ªå‘é‡é‡åŒ–å±‚çš„ token IDsï¼Œå½¢çŠ¶ä¸º (batch_size, sequence_length)
                            
                            # self.emb_code[i] æ˜¯ç¬¬ i ä¸ªåµŒå…¥å±‚ï¼Œå®ƒå°† input_ids_i è½¬æ¢ä¸ºåµŒå…¥è¡¨ç¤º
                            emb_i = self.emb_code[i](input_ids_i)   # emb_i çš„å½¢çŠ¶ä¸º (batch_size, sequence_length, embedding_dim)
                            # å°†æ¯ä¸ªå‘é‡é‡åŒ–å±‚çš„åµŒå…¥è¡¨ç¤ºå­˜å‚¨åœ¨ code_emb åˆ—è¡¨ä¸­
                            code_emb.append(emb_i)

                        # å †å åçš„å½¢çŠ¶ä¸º (batch_size, sequence_length, embedding_dim, num_vq)
                        stacked_emb = torch.stack(code_emb, 3)
                        
                        # å¯¹å †å åçš„å¼ é‡åœ¨ç¬¬ 3 ç»´åº¦ä¸Šè¿›è¡Œæ±‚å’Œ
                        final_emb = stacked_emb.sum(3)
                        
                        # å°†æœ€ç»ˆçš„åµŒå…¥è¡¨ç¤ºæ·»åŠ åˆ° model_input å­—å…¸ä¸­
                        model_input['inputs_embeds'] = final_emb
                        
                        # è§£é‡Šï¼šå°†å¤šä¸ªåµŒå…¥è¡¨ç¤ºæ²¿æ–°çš„ç»´åº¦å †å èµ·æ¥ï¼Œå¹¶åœ¨è¯¥ç»´åº¦ä¸Šæ±‚å’Œï¼Œç”Ÿæˆä¸€ä¸ªç»¼åˆçš„åµŒå…¥è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•å¯ä»¥å°†ä¸åŒå±‚æ¬¡çš„ä¿¡æ¯èåˆåœ¨ä¸€èµ·ï¼Œæä¾›æ›´ä¸°å¯Œçš„è¾“å…¥è¡¨ç¤º
                        # åœ¨ä¸€äº›é«˜çº§åº”ç”¨ä¸­ï¼Œå¯èƒ½ä¼šä½¿ç”¨å¤šå±‚åµŒå…¥è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼š
                        # åœ¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä¸åŒçš„å¤´å¯ä»¥çœ‹ä½œæ˜¯ä¸åŒçš„å±‚ï¼Œæ¯ä¸ªå¤´æ•æ‰è¾“å…¥åºåˆ—çš„ä¸åŒæ–¹é¢çš„ä¿¡æ¯
                        # åœ¨å¤šä»»åŠ¡å­¦ä¹ ä¸­ï¼Œä¸åŒçš„ä»»åŠ¡å¯èƒ½éœ€è¦ä¸åŒçš„åµŒå…¥è¡¨ç¤ºï¼Œé€šè¿‡å¤šå±‚åµŒå…¥è¡¨ç¤ºå¯ä»¥ä¸ºæ¯ä¸ªä»»åŠ¡ç”Ÿæˆç‰¹å®šçš„åµŒå…¥è¡¨ç¤º
                        # åœ¨ä¸€äº›ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œå‘é‡é‡åŒ–æŠ€æœ¯ç”¨äºå°†è¿ç»­çš„åµŒå…¥è¡¨ç¤ºç¦»æ•£åŒ–ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ•æ‰è¾“å…¥åºåˆ—çš„ç»“æ„ä¿¡æ¯
                
                # ç”±äºæˆ‘ä»¬å·²ç»ç”Ÿæˆäº†åµŒå…¥è¡¨ç¤ºå¹¶å°†å…¶å­˜å‚¨åœ¨ model_input['inputs_embeds'] ä¸­ï¼Œå› æ­¤ä¸å†éœ€è¦ input_ids
                model_input['input_ids'] = None
                # è°ƒç”¨ GPT æ¨¡å‹çš„ forward æ–¹æ³•è¿›è¡Œå‰å‘ä¼ æ’­ã€‚
                outputs = self.gpt.forward(**model_input, output_attentions=return_attn)
                # å°†æ³¨æ„åŠ›æƒé‡å­˜å‚¨åˆ° attentions åˆ—è¡¨ä¸­ï¼Œä»¥ä¾¿åç»­æ“ä½œ
                attentions.append(outputs.attentions)
                # è·å–æ¨¡å‹çš„è¾“å‡ºéšè—çŠ¶æ€ï¼Œå½¢çŠ¶ä¸º (batch_size, sequence_length, hidden_size)
                # è¿™äº›éšè—çŠ¶æ€åŒ…å«äº†æ¨¡å‹å¯¹è¾“å…¥åºåˆ—çš„è¡¨ç¤º
                # å­˜å‚¨éšè—çŠ¶æ€å¯ä»¥ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚åˆ†ç±»ã€ç”Ÿæˆç­‰
                hidden_states = outputs[0] # ğŸ»
                if return_hidden:
                    hiddens.append(hidden_states[:, -1])

                with P.cached():
                    if infer_text:  
                        logits = self.head_text(hidden_states) 
                    else:
                        # ä½¿ç”¨ self.head_code çš„å¤šä¸ªå¤´ï¼ˆself.num_vqï¼‰å¤„ç†éšè—çŠ¶æ€ï¼Œå¹¶å°†ç»“æœå †å åœ¨ä¸€èµ·ï¼Œç”Ÿæˆä»£ç çš„ logits
                        logits = torch.stack([self.head_code[i](hidden_states) for i in range(self.num_vq)], 3)
        
                # è·å–æ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ª token çš„ logitsï¼Œå½¢çŠ¶ä¸º (batch_size, num_classes)
                # å°† logits è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œä»¥ç¡®ä¿åç»­è®¡ç®—çš„ç²¾åº¦
                logits = logits[:, -1].float()

                if not infer_text:
                    # logits é‡æ–°æ’åˆ—ä¸ºå½¢çŠ¶ (batch_size * num_vq, num_classes)
                    logits = rearrange(logits, "b c n -> (b n) c")
                    #  é‡æ–°æ’åˆ—ä¸ºå½¢çŠ¶ (batch_size * num_vq, num_classes)
                    logits_token = rearrange(inputs_ids[:, start_idx:], "b c n -> (b n) c")
                else:
                    logits_token = inputs_ids[:, start_idx:, 0]
                
                # å°† logits é™¤ä»¥æ¸©åº¦å‚æ•° temperatureï¼Œä»¥æ§åˆ¶æ¨¡å‹è¾“å‡ºçš„å¹³æ»‘åº¦
                # è¾ƒé«˜çš„æ¸©åº¦ä¼šä½¿åˆ†å¸ƒæ›´åŠ å¹³æ»‘ï¼Œè¾ƒä½çš„æ¸©åº¦ä¼šä½¿åˆ†å¸ƒæ›´åŠ å°–é”ã€‚
                logits = logits / temperature
                
                # é‡å¤æƒ©ç½šå™¨ï¼Œé¿å…é‡å¤è¾“å‡ºåŒä¸€ä¸ªå­—è¯
                for logitsProcessors in LogitsProcessors:
                    logits = logitsProcessors(logits_token, logits)
                
                # # top-k,top-p,temperate ç­‰å¤šä¸ªå‚æ•°åº”ç”¨
                # 1. top-kï¼šåªä¿ç•™æ¦‚ç‡æœ€é«˜çš„ top-k ä¸ªå€™é€‰è¯ï¼Œå…¶ä»–å€™é€‰è¯çš„æ¦‚ç‡è¢«è®¾ç½®ä¸º 0ã€‚è¿™æ ·å¯ä»¥é™åˆ¶æ¨¡å‹ç”Ÿæˆçš„ç»“æœåªåŒ…å«æ¦‚ç‡æœ€é«˜çš„ top-k ä¸ªå€™é€‰è¯ã€‚
                # 2. top-pï¼šåªä¿ç•™æ¦‚ç‡ä¹‹å’Œä¸å¤§äº top-p çš„å€™é€‰è¯ï¼Œå…¶ä»–å€™é€‰è¯çš„æ¦‚ç‡è¢«è®¾ç½®ä¸º 0ã€‚è¿™æ ·å¯ä»¥é™åˆ¶æ¨¡å‹ç”Ÿæˆçš„ç»“æœåªåŒ…å«æ¦‚ç‡ä¹‹å’Œä¸å¤§äº top-p çš„å€™é€‰è¯ã€‚
                # 3. temperateï¼šæ§åˆ¶æ¨¡å‹è¾“å‡ºçš„å¹³æ»‘åº¦ï¼Œtemperature è¶Šå¤§è¾“å‡ºè¶Šå¤šæ ·æ€§ï¼Œtemperature è¶Šå°è¾“å‡ºè¶Šç›¸ä¼¼ã€‚
                for logitsWarpers in LogitsWarpers:
                    logits = logitsWarpers(logits_token, logits)
                
                # å¦‚æœå½“å‰ç”Ÿæˆçš„ token æ•°é‡å°äº min_new_tokenï¼Œåˆ™å°† eos_token çš„ logits è®¾ç½®ä¸ºè´Ÿæ— ç©·å¤§ã€‚
                # è¿™æ ·å¯ä»¥é˜²æ­¢æ¨¡å‹åœ¨ç”Ÿæˆè¶³å¤Ÿçš„æ–° token ä¹‹å‰æå‰ç»“æŸã€‚
                if i < min_new_token:
                    logits[:, eos_token] = -torch.inf
                
                # åº”ç”¨ softmax å‡½æ•°ï¼Œå¾—åˆ°æ¯ä¸ª token çš„æ¦‚ç‡åˆ†å¸ƒ
                scores = F.softmax(logits, dim=-1)
            
                # ä½¿ç”¨å¤šé¡¹å¼åˆ†å¸ƒä»åˆ†æ•°ä¸­é‡‡æ ·ä¸‹ä¸€ä¸ª token çš„ç´¢å¼•
                # num_samples=1 è¡¨ç¤ºæ¯æ¬¡é‡‡æ ·ä¸€ä¸ª token
                idx_next = torch.multinomial(scores, num_samples=1)
                
                if not infer_text:
                    # idx_next é‡æ–°æ’åˆ—ä¸ºå½¢çŠ¶ (batch_size, num_vq)
                    idx_next = rearrange(idx_next, "(b n) 1 -> b n", n=self.num_vq)
                    # æ ‡è®°æ›´æ–°ä¸ºæ˜¯å¦æœ‰ä»»ä½•åºåˆ—ç”Ÿæˆäº† eos_token
                    finish = finish | (idx_next == eos_token).any(1)
                    
                    # inputs_ids æ˜¯å½“å‰ç”Ÿæˆçš„åºåˆ—å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, sequence_length),å®ƒåŒ…å«äº†æ¨¡å‹å·²ç»ç”Ÿæˆçš„æ‰€æœ‰ token çš„ç´¢å¼•
                    # idx_next æ˜¯ä»æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·å¾—åˆ°çš„ä¸‹ä¸€ä¸ª token çš„ç´¢å¼•ï¼Œå½¢çŠ¶ä¸º (batch_size, 1),å®ƒè¡¨ç¤ºæ¯ä¸ªåºåˆ—åœ¨å½“å‰æ­¥éª¤ç”Ÿæˆçš„ä¸‹ä¸€ä¸ª token
                    # unsqueeze æ˜¯ PyTorch ä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºåœ¨æŒ‡å®šä½ç½®æ’å…¥ä¸€ä¸ªæ–°çš„ç»´åº¦
                    # idx_next.unsqueeze(1) å°† idx_next çš„å½¢çŠ¶ä» (batch_size, 1) å˜ä¸º (batch_size, 1, 1)
                    # torch.cat å°†è¿™ä¸¤ä¸ªå¼ é‡è¿›è¡Œæ‹¼æ¥ï¼Œå¾—åˆ°æ–°çš„å¼ é‡å½¢çŠ¶ä¸º (batch_size, sequence_length + 1),è¡¨ç¤ºåœ¨å½“å‰ç”Ÿæˆçš„åºåˆ—æœ«å°¾æ·»åŠ äº†æ–°ç”Ÿæˆçš„ tokenã€‚
                    inputs_ids = torch.cat([inputs_ids, idx_next.unsqueeze(1)], 1)
                else:
                    finish = finish | (idx_next == eos_token).any(1)
                    inputs_ids = torch.cat([inputs_ids, idx_next.unsqueeze(-1).expand(-1, -1, self.num_vq)], 1)

                # æ›´æ–° `end_idx` è®°å½•æ¯ä¸ªæ ·æœ¬çš„ç»“æŸä½ç½®
                end_idx = end_idx + (~finish).int()

                # å¦‚æœæ‰€æœ‰æ ·æœ¬éƒ½å®Œæˆç”Ÿæˆï¼Œé€€å‡ºå¾ªç¯
                if finish.all():
                    break
            
            #  æ ¹æ® `end_idx` æˆªå–ç”Ÿæˆçš„ `inputs_ids`
            # inputs_ids = [inputs_ids[idx, start_idx: start_idx+i] for idx, i in enumerate(end_idx.int())]
            # inputs_ids = [i[:, 0] for i in inputs_ids] if infer_text else inputs_ids
            # åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œè¿™æ®µä»£ç çš„ä½œç”¨æ˜¯æ ¹æ®æ¯ä¸ªåºåˆ—çš„ç»“æŸä½ç½®å¯¹è¾“å…¥åºåˆ—è¿›è¡Œåˆ‡ç‰‡ï¼Œ
            # ä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†å˜é•¿çš„è¾“å…¥åºåˆ—ã€‚è¿™åœ¨ç”Ÿæˆä»»åŠ¡ä¸­éå¸¸é‡è¦ï¼Œå› ä¸ºç”Ÿæˆçš„åºåˆ—é•¿åº¦å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒ
            inputs_ids_list = []
            for idx, i in enumerate(end_idx.int()):
                sliced_input = inputs_ids[idx, start_idx: start_idx+i]
                inputs_ids_list.append(sliced_input)
            inputs_ids = inputs_ids_list
            # å±•å¼€åçš„ä»£ç 
            if infer_text:
                inputs_ids_list = []
                for i in inputs_ids:
                    sliced_input = i[:, 0]
                    inputs_ids_list.append(sliced_input)
                inputs_ids = inputs_ids_list
            
            # å¦‚æœ `return_hidden` ä¸ºçœŸï¼Œè¿”å›éšè—çŠ¶æ€
            if return_hidden:
                # å°† hiddens åˆ—è¡¨ä¸­çš„å¼ é‡æ²¿ç»´åº¦ 1 è¿›è¡Œå †å ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„å¼ é‡ã€‚
                # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼å¯¹ hiddens è¿›è¡Œåˆ‡ç‰‡æ“ä½œï¼Œç±»ä¼¼äºå¤„ç† inputs_ids çš„ç¬¬ä¸€æ­¥
                hiddens = torch.stack(hiddens, 1)
                # hiddens = [hiddens[idx, :i] for idx, i in enumerate(end_idx.int())]
                hiddens_list = []
                for idx, i in enumerate(end_idx.int()):
                    sliced_hidden = hiddens[idx, :i]
                    hiddens_list.append(sliced_hidden)
                hiddens = hiddens_list
                    
            if not finish.all():
                self.logger.warn(f'Incomplete result. hit max_new_token: {max_new_token}')    
                   
            return {
                'ids': inputs_ids, 
                'attentions': attentions,
                'hiddens':hiddens,
            }