{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 07:32:56.076778: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-18 07:32:57.084765: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from modules.wavenet import WaveNet\n",
    "from modules.dvae import GFSQ, DVAEDecoder\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import librosa\n",
    "import torchaudio\n",
    "from torch.utils.data import random_split\n",
    "import logging\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from modules.feature_extractors import MelSpectrogramFeatures\n",
    "from modules.discriminator import DynamicAudioDiscriminator,Discriminator,DynamicAudioDiscriminatorWithResidual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置设备，优先使用CUDA，其次是MPS（Mac上的GPU加速），最后是CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Use device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "# 设置日志级别为INFO\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.info(f\"Use device: {device}\")\n",
    "log_dir = \"runs/experiment1\"  # 指定日志目录\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_files, sample_rate=24000,n_fft =1024,hop_length=512,n_mels=100):\n",
    "        # 初始化音频文件列表和Mel谱图转换器\n",
    "        self.audio_files = audio_files\n",
    "        # self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate)\n",
    "        self.mel_spectrogram = MelSpectrogramFeatures(sample_rate=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "        self.sample_rate = sample_rate\n",
    "    def __len__(self):\n",
    "        # 返回数据集中的音频文件数量\n",
    "        return len(self.audio_files)\n",
    "    def __getitem__(self, idx):\n",
    "        # 加载并返回指定索引的音频文件的Mel谱图\n",
    "        mel_spectrogram = self.load_mel_spectrogram(self.audio_files[idx])\n",
    "        return mel_spectrogram\n",
    "    # def load_mel_spectrogram(self, file_path):\n",
    "    #     # 加载音频文件并转换为Mel谱图\n",
    "    #     waveform, sr = librosa.load(file_path, sr=self.sample_rate, mono=True)\n",
    "    #     S = librosa.feature.melspectrogram(y=waveform, sr=sr, n_mels=128,n_fft=1024,hop_length=256,)\n",
    "    #     return torch.from_numpy(S)\n",
    "    def load_mel_spectrogram(self, file_path):\n",
    "        # 加载音频文件并转换为Mel谱图\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0)\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
    "        mel_spectrogram = self.mel_spectrogram(waveform)\n",
    "        return mel_spectrogram[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_files(root_dir):\n",
    "    \"\"\"# 从指定目录加载所有符合条件的音频文件\n",
    "    \"\"\"\n",
    "    audio_files = []\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                duration = torchaudio.info(file_path).num_frames / torchaudio.info(file_path).sample_rate\n",
    "                if 1 <= duration <= 30:\n",
    "                    audio_files.append(file_path)\n",
    "    return audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_collate_fn(batch):\n",
    "    # Filter out tensors that do not have 2 dimensions\n",
    "    batch = [tensor for tensor in batch if len(tensor.shape) == 2]\n",
    "\n",
    "    # If the batch is empty after filtering, return None to skip this batch\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    \n",
    "    # 按照音频长度排序\n",
    "    batch.sort(key=lambda x: x.shape[1], reverse=True)\n",
    "    max_len = batch[0].shape[1]\n",
    "    \n",
    "    # 填充所有张量到相同的长度\n",
    "    padded_batch = []\n",
    "    for tensor in batch:\n",
    "        padded_tensor = torch.nn.functional.pad(tensor, (0, max_len - tensor.shape[1]), mode='constant', value=0)\n",
    "        padded_batch.append(padded_tensor)\n",
    "    \n",
    "    batch_tensor = torch.stack(padded_batch)\n",
    "    return batch_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"WaveNet\": {\"input_channels\": 100, \"output_channels\": 1024, 'residual_layers': 20, 'dilation_cycle': 4,},\n",
    "    \"GFSQ\": {\"dim\": 1024, \"levels\": [8,5,5,5], \"G\": 2, \"R\": 2},\n",
    "    \"DVAEDecoder\": {\"idim\": 1024, \"odim\": 100, \"n_layer\":12, \"bn_dim\": 128, \"hidden\":1024},\n",
    "    \"Discriminator\":{}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet = WaveNet(**model_params[\"WaveNet\"]).to(device)\n",
    "gfsq = GFSQ(**model_params[\"GFSQ\"]).to(device)\n",
    "decoder = DVAEDecoder(**model_params[\"DVAEDecoder\"]).to(device)\n",
    "# 初始化判别器\n",
    "discriminator = DynamicAudioDiscriminatorWithResidual(**model_params[\"Discriminator\"]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = 'MSE'\n",
    "if loss_type == 'MSE':\n",
    "    criterion = nn.MSELoss()\n",
    "else:\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    list(wavenet.parameters()) + list(gfsq.parameters()) + list(decoder.parameters()), \n",
    "    lr=4e-4,\n",
    "    betas=(0.8, 0.99),\n",
    "    eps=1e-6,\n",
    ")\n",
    "# 定义判别器的优化器\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=5e-5,eps=1e-6,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用学习率调度器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999999)  # 调整调度器参数\n",
    "# 在模型定义后添加以下代码\n",
    "T_max = 100  # 余弦退火的最大周期为总轮数\n",
    "eta_min = 1e-6  # 最小学习率为1e-6\n",
    "\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "scheduler_d = optim.lr_scheduler.CosineAnnealingLR(optimizer_d, T_max=T_max, eta_min=eta_min)  # 调整调度器参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度累积设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据集并拆分为训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/tmp/three_moon\"\n",
    "# root_dir = \"/root/autodl-tmp/VCTK-Corpus\"\n",
    "audio_files = get_audio_files(root_dir)\n",
    "dataset = AudioDataset(audio_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切割分成训练集和校验集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train size: 2818 \t Val size: 149\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.95 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "logger.info(f\"Train size: {len(train_dataset)} \\t Val size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cuda' in str(device):\n",
    "    batch_size = 8\n",
    "else:\n",
    "    batch_size = 1\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=dynamic_collate_fn, )\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=dynamic_collate_fn, )\n",
    "\n",
    "# 创建 GradScaler，转换为fp16\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查找是否有记录点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob  # 用于查找模型文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义 resume 变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = False  # 如果需要从最新检查点恢复训练，则设置为 True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取最新的检查点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state_dict_to_float(state_dict):\n",
    "    \"\"\"\n",
    "    将 state_dict 中的所有张量从 fp16 转换为 fp32\n",
    "    \"\"\"\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        new_state_dict[k] = v.float()  # 将每个张量转换为float32\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "if resume:\n",
    "    checkpoint_files = glob.glob('checkpoint_epoch_*.pth')\n",
    "    if checkpoint_files:\n",
    "        latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
    "        checkpoint = torch.load(latest_checkpoint)\n",
    "        wavenet.load_state_dict(convert_state_dict_to_float(checkpoint['wavenet_state_dict']))\n",
    "        gfsq.load_state_dict(convert_state_dict_to_float(checkpoint['gfsq_state_dict']))\n",
    "        decoder.load_state_dict(convert_state_dict_to_float(checkpoint['decoder_state_dict']))\n",
    "        discriminator.load_state_dict(convert_state_dict_to_float(checkpoint['discriminator_state_dict']))\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        optimizer_d.load_state_dict(checkpoint['optimizer_d_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        logger.info(f\"Resumed training from epoch {start_epoch}\")\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        logger.info(\"No checkpoint found, starting from scratch.\")\n",
    "else:\n",
    "    start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def mel_to_audio(mel_spectrogram, sr=24000, n_fft=1024, hop_length=256, win_length=None):\n",
    "    \"\"\"将 Mel 频谱图转换回音频信号\"\"\"\n",
    "    # 确保输入为 NumPy 数组\n",
    "    if isinstance(mel_spectrogram, torch.Tensor):\n",
    "        mel_spectrogram = mel_spectrogram.cpu().numpy()\n",
    "    \n",
    "    # 使用 librosa 的功能进行逆 Mel 频谱变换\n",
    "    mel_decompress = librosa.feature.inverse.mel_to_audio(mel_spectrogram, sr=sr, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n",
    "    return mel_decompress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义混合损失函数\n",
    "def mixed_loss(decoded_features, mel_spectrogram):\n",
    "    loss_mse = F.mse_loss(decoded_features, mel_spectrogram)\n",
    "    loss_l1 = F.l1_loss(decoded_features, mel_spectrogram)\n",
    "    return loss_mse*0.5 + 0.5 * loss_l1\n",
    "\n",
    "# 时间步翻倍\n",
    "def double_time_steps(mel_spectrogram):\n",
    "    batch_size, n_mels, time_steps = mel_spectrogram.shape\n",
    "    mel_spectrogram = mel_spectrogram.unsqueeze(1)  # 添加通道维度\n",
    "    doubled_mel = F.interpolate(mel_spectrogram, size=(n_mels, time_steps * 2), mode='bilinear', align_corners=False)\n",
    "    return doubled_mel.squeeze(1)  # 移除通道维度\n",
    "\n",
    "# 定义判别器的损失函数\n",
    "def discriminator_loss_fn(outputs_real, outputs_fake):\n",
    "    real_loss = F.binary_cross_entropy_with_logits(outputs_real, torch.ones_like(outputs_real))\n",
    "    fake_loss = F.binary_cross_entropy_with_logits(outputs_fake, torch.zeros_like(outputs_fake))\n",
    "    return (real_loss + fake_loss) / 2\n",
    "\n",
    "# 定义生成器的损失函数\n",
    "def generator_loss_fn(outputs_fake):\n",
    "    return F.binary_cross_entropy_with_logits(outputs_fake, torch.ones_like(outputs_fake))\n",
    "\n",
    "# 定义判别器损失函数\n",
    "criterion_d = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(wavenet, gfsq, decoder, val_loader, device):\n",
    "    wavenet.eval()\n",
    "    gfsq.eval()\n",
    "    decoder.eval()\n",
    "    val_loss_mse = 0\n",
    "    val_loss_l1 = 0\n",
    "    with torch.no_grad():\n",
    "        for mel_spectrogram in val_loader:\n",
    "            if mel_spectrogram is None:\n",
    "                continue  # Skip this batch\n",
    "            \n",
    "            mel_spectrogram = mel_spectrogram.to(device)\n",
    "            features = wavenet(mel_spectrogram)\n",
    "            _, quantized_features, _, _, _ = gfsq(features)\n",
    "            decoded_features = decoder(quantized_features)\n",
    "            loss_mse = F.mse_loss(decoded_features, mel_spectrogram)\n",
    "            loss_l1 = F.l1_loss(decoded_features, mel_spectrogram)\n",
    "            val_loss_mse += loss_mse.item()\n",
    "            val_loss_l1 += loss_l1.item()\n",
    "    val_loss_mse /= len(val_loader)\n",
    "    val_loss_l1 /= len(val_loader)\n",
    "    return val_loss_mse, val_loss_l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:判别器损失过低，结束当前epoch的判别器训练\n",
      "INFO:root:Epoch [1/10000], Step [100], Generator Loss: 0.670278012752533, Perplexity: 17.29658317565918\n",
      "INFO:root:Epoch [1/10000], Step [200], Generator Loss: 0.691554844379425, Perplexity: 20.255817413330078\n",
      "INFO:root:Epoch [1/10000], Step [235], Generator Loss: 0.5871320962905884, Perplexity: 36.890174865722656\n",
      "INFO:root:Epoch [1/10000], MSE Loss: 0.40745930717541623, L1 Loss: 0.34652363337003267\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 346.00 MiB. GPU 0 has a total capacity of 15.74 GiB of which 304.69 MiB is free. Process 618806 has 15.44 GiB memory in use. Of the allocated memory 10.34 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m real_loss \u001b[38;5;241m=\u001b[39m criterion_d(real_output, real_labels)\n\u001b[1;32m     32\u001b[0m features \u001b[38;5;241m=\u001b[39m wavenet(mel_spectrogram)\n\u001b[0;32m---> 33\u001b[0m _, quantized_features, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mgfsq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m decoded_features \u001b[38;5;241m=\u001b[39m decoder(quantized_features)\n\u001b[1;32m     35\u001b[0m fake_output \u001b[38;5;241m=\u001b[39m discriminator(decoded_features\u001b[38;5;241m.\u001b[39mdetach())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/autodl-tmp/vq_encoder/modules/dvae.py:103\u001b[0m, in \u001b[0;36mGFSQ.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     99\u001b[0m ind \u001b[38;5;241m=\u001b[39m rearrange(\n\u001b[1;32m    100\u001b[0m     ind, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg b t r ->b t (g r)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    101\u001b[0m )  \n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# 对量化索引进行One-hot编码，这是为了计算每个量化级别的使用频率，进而计算模型的多样性（即复杂度\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m embed_onehot \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_ind\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# 计算One-hot编码的均值，然后用这个均值来计算每个量化级别的使用频率\u001b[39;00m\n\u001b[1;32m    105\u001b[0m e_mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(embed_onehot, dim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 346.00 MiB. GPU 0 has a total capacity of 15.74 GiB of which 304.69 MiB is free. Process 618806 has 15.44 GiB memory in use. Of the allocated memory 10.34 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# 训练循环\n",
    "num_epochs = 10000  # 定义训练的总轮数\n",
    "step_counter = 0  # 步骤计数器\n",
    "step_counter_d = 0  # 判别器步骤计数器\n",
    "step_counter_g = 0  # 生成器步骤计数器\n",
    "discriminator_train_steps = 3 # 判别器先训练，然后再轮到生成器\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    # 判别器训练循环\n",
    "    discriminator.train()\n",
    "    wavenet.eval()\n",
    "    gfsq.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    for mel_spectrogram in train_loader:\n",
    "        if mel_spectrogram is None:\n",
    "            continue  # Skip this batch\n",
    "\n",
    "        mel_spectrogram = mel_spectrogram.to(device)\n",
    "\n",
    "        # 更新判别器\n",
    "        optimizer_d.zero_grad()\n",
    "        with autocast():\n",
    "            real_labels = torch.ones(mel_spectrogram.size(0), device=device)\n",
    "            fake_labels = torch.zeros(mel_spectrogram.size(0), device=device)\n",
    "\n",
    "            real_output = discriminator(mel_spectrogram)\n",
    "            real_output = real_output.view(-1)\n",
    "            real_loss = criterion_d(real_output, real_labels)\n",
    "\n",
    "            features = wavenet(mel_spectrogram)\n",
    "            _, quantized_features, _, _, _ = gfsq(features)\n",
    "            decoded_features = decoder(quantized_features)\n",
    "            fake_output = discriminator(decoded_features.detach())\n",
    "            fake_output = fake_output.view(-1)\n",
    "            fake_loss = criterion_d(fake_output, fake_labels)\n",
    "\n",
    "            d_loss = real_loss + fake_loss\n",
    "\n",
    "        scaler.scale(d_loss).backward()\n",
    "        scaler.step(optimizer_d)\n",
    "        scaler.update()\n",
    "\n",
    "        # 记录判别器的损失\n",
    "        writer.add_scalar('Discriminator Loss/real', real_loss.item(), step_counter_d)\n",
    "        writer.add_scalar('Discriminator Loss/fake', fake_loss.item(), step_counter_d)\n",
    "        writer.add_scalar('Discriminator Loss/total', d_loss.item(), step_counter_d)\n",
    "        \n",
    "        step_counter_d += 1\n",
    "        # 检查早停条件\n",
    "        if d_loss.item() < 0.1:\n",
    "            logger.info(\"判别器损失过低，结束当前epoch的判别器训练\")\n",
    "            break\n",
    "            \n",
    "        # 每100步打印日志\n",
    "        if (step_counter_d + 1) % 100 == 0 or (step_counter_d + 1) == len(train_loader):\n",
    "            logger.info(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step_counter_d+1}], Discriminator Loss: {d_loss.item()}\")\n",
    "\n",
    "        # 每50步释放不必要的张量\n",
    "        if (step_counter_d + 1) % 50 == 0:\n",
    "            del real_output, real_loss, fake_output, fake_loss, d_loss\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # 生成器训练循环\n",
    "    wavenet.train()\n",
    "    gfsq.train()\n",
    "    decoder.train()\n",
    "    discriminator.eval()\n",
    "    \n",
    "    for mel_spectrogram in train_loader:\n",
    "        if mel_spectrogram is None:\n",
    "            continue  # Skip this batch\n",
    "\n",
    "        mel_spectrogram = mel_spectrogram.to(device)\n",
    "\n",
    "        # 更新生成器\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            features = wavenet(mel_spectrogram)\n",
    "            _, quantized_features, perplexity, _, quantized_indices = gfsq(features)\n",
    "            decoded_features = decoder(quantized_features)\n",
    "\n",
    "            g_loss = criterion(decoded_features, mel_spectrogram)\n",
    "            g_output = discriminator(decoded_features)\n",
    "            g_output = g_output.view(-1)\n",
    "            real_labels = torch.ones(mel_spectrogram.size(0), device=device)\n",
    "            g_adv_loss = criterion_d(g_output, real_labels)\n",
    "\n",
    "            total_g_loss = g_loss + g_adv_loss * 0.3\n",
    "\n",
    "        scaler.scale(total_g_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # 记录生成器的损失\n",
    "        writer.add_scalar('Generator Loss/total', total_g_loss.item(), step_counter_g)\n",
    "        writer.add_scalar('Generator Loss/reconstruction', g_loss.item(), step_counter_g)\n",
    "        writer.add_scalar('Generator Loss/adversarial', g_adv_loss.item(), step_counter_g)\n",
    "        writer.add_scalar('training_perplexity', perplexity.mean().item(), step_counter_g)\n",
    "\n",
    "        step_counter_g += 1\n",
    "        \n",
    "        # 每100步打印日志\n",
    "        if (step_counter_g + 1) % 100 == 0 or (step_counter_g + 1) == len(train_loader):\n",
    "            logger.info(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step_counter_g+1}], Generator Loss: {total_g_loss.item()}, Perplexity: {perplexity.mean().item()}\")\n",
    "\n",
    "        # 每50步释放不必要的张量\n",
    "        if (step_counter_g + 1) % 50 == 0:\n",
    "            del features, quantized_features, decoded_features, total_g_loss, perplexity\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # 调整学习率\n",
    "    scheduler_d.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # 验证模型\n",
    "    val_loss_mse, val_loss_l1 = evaluate(wavenet, gfsq, decoder, val_loader, device)\n",
    "    logger.info(f\"Epoch [{epoch+1}/{num_epochs}], MSE Loss: {val_loss_mse}, L1 Loss: {val_loss_l1}\")\n",
    "    writer.add_scalar('validation_mse_loss', val_loss_mse, epoch)\n",
    "    writer.add_scalar('validation_l1_loss', val_loss_l1, epoch)\n",
    "\n",
    "    # 每5个epoch就保存模型状态字典\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint_path = f'checkpoint_epoch_{epoch+1}.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'wavenet_state_dict': wavenet.state_dict(),\n",
    "            'gfsq_state_dict': gfsq.state_dict(), \n",
    "            'decoder_state_dict': decoder.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "        }, checkpoint_path)\n",
    "        logger.info(f\"Model saved to {checkpoint_path}\")\n",
    "    \n",
    "logger.info(\"训练完成\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 训练循环\n",
    "num_epochs = 10000  # 定义训练的总轮数\n",
    "step_counter = 0  # 步骤计数器\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    wavenet.train()\n",
    "    gfsq.train()\n",
    "    decoder.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for mel_spectrogram in train_loader:\n",
    "        if mel_spectrogram is None:\n",
    "            continue  # Skip this batch\n",
    "        \n",
    "        mel_spectrogram = mel_spectrogram.to(device)\n",
    "        \n",
    "        # 更新判别器\n",
    "        optimizer_d.zero_grad()\n",
    "        with autocast():\n",
    "            real_labels = torch.ones(mel_spectrogram.size(0), device=device)\n",
    "            fake_labels = torch.zeros(mel_spectrogram.size(0), device=device)\n",
    "            \n",
    "            real_output = discriminator(mel_spectrogram)\n",
    "            real_output = real_output.view(-1)\n",
    "            real_loss = criterion_d(real_output, real_labels)\n",
    "            \n",
    "            features = wavenet(mel_spectrogram)\n",
    "            _, quantized_features, _, _, _ = gfsq(features)\n",
    "            decoded_features = decoder(quantized_features)\n",
    "            fake_output = discriminator(decoded_features.detach())\n",
    "            fake_output = fake_output.view(-1)\n",
    "            fake_loss = criterion_d(fake_output, fake_labels)\n",
    "            \n",
    "            d_loss = (real_loss + fake_loss) /2 \n",
    "        \n",
    "        scaler.scale(d_loss).backward()\n",
    "        scaler.step(optimizer_d)\n",
    "        scaler.update()\n",
    "        \n",
    "        # 记录判别器的损失\n",
    "        writer.add_scalar('Discriminator Loss/real', real_loss.item(), step_counter)\n",
    "        writer.add_scalar('Discriminator Loss/fake', fake_loss.item(), step_counter)\n",
    "        writer.add_scalar('Discriminator Loss/total', d_loss.item(), step_counter)\n",
    "        \n",
    "        # 更新生成器\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            features = wavenet(mel_spectrogram)\n",
    "            _, quantized_features, perplexity, _, quantized_indices = gfsq(features)# 通过GFSQ量化特征\n",
    "            decoded_features = decoder(quantized_features)\n",
    "            \n",
    "            g_loss = criterion(decoded_features, mel_spectrogram)\n",
    "            g_output = discriminator(decoded_features)\n",
    "            g_output = g_output.view(-1)\n",
    "            g_adv_loss = criterion_d(g_output, real_labels)\n",
    "            \n",
    "            total_g_loss = g_loss + g_adv_loss * 0.3\n",
    "        \n",
    "        scaler.scale(total_g_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # 记录生成器的损失\n",
    "        writer.add_scalar('Generator Loss/total', total_g_loss.item(), step_counter)\n",
    "        writer.add_scalar('Generator Loss/reconstruction', g_loss.item(), step_counter)\n",
    "        writer.add_scalar('Generator Loss/adversarial', g_adv_loss.item(), step_counter)\n",
    "        writer.add_scalar('training_perplexity',perplexity.mean().item(),step_counter)\n",
    "        \n",
    "        if (step_counter + 1) % 100 == 0 or (step_counter + 1) == len(train_loader):\n",
    "            logger.info(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step_counter+1}], Discriminator Loss: {d_loss.item()}, Generator Loss: {total_g_loss.item()}, Perplexity: {perplexity.mean().item()}\")\n",
    "        \n",
    "        # 释放不必要的张量\n",
    "        if (step_counter + 1) % 50 == 0:\n",
    "            del real_output, real_loss, fake_output, fake_loss, d_loss, features, quantized_features, decoded_features, total_g_loss\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        step_counter += 1\n",
    "            \n",
    "    # 调整学习率\n",
    "    scheduler_d.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # 验证模型\n",
    "    val_loss_mse, val_loss_l1 = evaluate(wavenet, gfsq, decoder, val_loader, device)\n",
    "    logger.info(f\"Epoch [{epoch+1}/{num_epochs}], MSE Loss: {val_loss_mse}, L1 Loss: {val_loss_l1}\")\n",
    "    writer.add_scalar('validation_mse_loss', val_loss_mse, epoch)\n",
    "    writer.add_scalar('validation_l1_loss', val_loss_l1, epoch)\n",
    "\n",
    "    # 每5个epoch就保存模型状态字典\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint_path = f'checkpoint_epoch_{epoch+1}.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'wavenet_state_dict': wavenet.state_dict(),\n",
    "            'gfsq_state_dict': gfsq.state_dict(), \n",
    "            'decoder_state_dict': decoder.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "        }, checkpoint_path)\n",
    "        logger.info(f\"Model saved to {checkpoint_path}\")\n",
    "    \n",
    "logger.info(\"训练完成\")\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
