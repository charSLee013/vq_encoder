{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charslee/miniconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from modules.wavenet import WaveNet\n",
    "from modules.dvae import GFSQ, DVAEDecoder\n",
    "from modules.spectrogram import LogMelSpectrogram\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import librosa\n",
    "import torchaudio\n",
    "from torch.utils.data import random_split\n",
    "import logging\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置设备，优先使用CUDA，其次是MPS（Mac上的GPU加速），最后是CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Use device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "# 设置日志级别为INFO\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.info(f\"Use device: {device}\")\n",
    "log_dir = \"runs/experiment1\"  # 指定日志目录\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_files, sample_rate=44100):\n",
    "        # 初始化音频文件列表和Mel谱图转换器\n",
    "        self.audio_files = audio_files\n",
    "        # self.mel_spec = LogMelSpectrogram(\n",
    "        #     sample_rate=44100,\n",
    "        #     n_fft=2048,\n",
    "        #     win_length=2048,\n",
    "        #     hop_length=512,\n",
    "        #     n_mels=128,\n",
    "        #     f_min=0.0,\n",
    "        #     f_max=8000.0,\n",
    "        # )\n",
    "        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate)\n",
    "        self.sample_rate = sample_rate\n",
    "    def __len__(self):\n",
    "        # 返回数据集中的音频文件数量\n",
    "        return len(self.audio_files)\n",
    "    def __getitem__(self, idx):\n",
    "        # 加载并返回指定索引的音频文件的Mel谱图\n",
    "        mel_spectrogram = self.load_mel_spectrogram(self.audio_files[idx])\n",
    "        return mel_spectrogram\n",
    "    def load_mel_spectrogram(self, file_path):\n",
    "        # 加载音频文件并转换为Mel谱图\n",
    "        waveform, sr = librosa.load(file_path, sr=self.sample_rate, mono=True)\n",
    "        S = librosa.feature.melspectrogram(y=waveform, sr=sr, n_mels=128,n_fft=1024,hop_length=256,)\n",
    "        return torch.from_numpy(S)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_files(root_dir):\n",
    "    \"\"\"# 从指定目录加载所有符合条件的音频文件\n",
    "    \"\"\"\n",
    "    audio_files = []\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                duration = torchaudio.info(file_path).num_frames / torchaudio.info(file_path).sample_rate\n",
    "                if 2 <= duration <= 30:\n",
    "                    audio_files.append(file_path)\n",
    "    return audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_collate_fn(batch):\n",
    "    # 按照音频长度排序\n",
    "    batch.sort(key=lambda x: x.shape[1], reverse=True)\n",
    "    max_len = batch[0].shape[1]\n",
    "    \n",
    "    # 填充所有张量到相同的长度\n",
    "    padded_batch = []\n",
    "    for tensor in batch:\n",
    "        padded_tensor = torch.nn.functional.pad(tensor, (0, max_len - tensor.shape[1]), mode='constant', value=0)\n",
    "        padded_batch.append(padded_tensor)\n",
    "    \n",
    "    if len(padded_batch) == 0:\n",
    "        raise ValueError(\"All tensors in the batch were skipped. Check your data preprocessing.\")\n",
    "    \n",
    "    batch_tensor = torch.stack(padded_batch)\n",
    "    return batch_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"WaveNet\": {\"input_channels\": 128, \"output_channels\": 1024, 'residual_layers': 20, 'dilation_cycle': 4},\n",
    "    \"GFSQ\": {\"dim\": 1024, \"levels\": [8, 5, 5, 5], \"G\": 2, \"R\": 2},\n",
    "    \"DVAEDecoder\": {\"idim\": 1024, \"odim\": 128, \"n_layer\":12, \"bn_dim\": 128, \"hidden\":512}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet = WaveNet(**model_params[\"WaveNet\"]).to(device)\n",
    "gfsq = GFSQ(**model_params[\"GFSQ\"]).to(device)\n",
    "decoder = DVAEDecoder(**model_params[\"DVAEDecoder\"]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = 'L1'\n",
    "if loss_type == 'MSE':\n",
    "    criterion = nn.MSELoss()\n",
    "else:\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    list(wavenet.parameters()) + list(gfsq.parameters()) + list(decoder.parameters()), \n",
    "    lr=1e-4, \n",
    "    betas=(0.8, 0.99)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用学习率调度器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)  # 调整调度器参数\n",
    "# 在模型定义后添加以下代码\n",
    "T_max = 100  # 余弦退火的最大周期为总轮数\n",
    "eta_min = 1e-6  # 最小学习率为1e-6\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度累积设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据集并拆分为训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charslee/miniconda3/envs/torch/lib/python3.10/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"/tmp/three_moon\"\n",
    "audio_files = get_audio_files(root_dir)\n",
    "dataset = AudioDataset(audio_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切割分成训练集和校验集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train size: 1988 \t Val size: 221\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "logger.info(f\"Train size: {len(train_dataset)} \\t Val size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cuda' in str(device):\n",
    "    batch_size = 8\n",
    "else:\n",
    "    batch_size = 1\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=dynamic_collate_fn, )\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=dynamic_collate_fn, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查找是否有记录点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob  # 用于查找模型文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义 resume 变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = True  # 如果需要从最新检查点恢复训练，则设置为 True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取最新的检查点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:No checkpoint found, starting from scratch.\n",
      "/Users/charslee/miniconda3/envs/torch/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def convert_state_dict_to_float(state_dict):\n",
    "    \"\"\"\n",
    "    将 state_dict 中的所有张量从 fp16 转换为 fp32\n",
    "    \"\"\"\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        new_state_dict[k] = v.float()  # 将每个张量转换为float32\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "if resume:\n",
    "    checkpoint_files = glob.glob('checkpoint_epoch_*.pth')\n",
    "    if checkpoint_files:\n",
    "        latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
    "        checkpoint = torch.load(latest_checkpoint)\n",
    "        wavenet.load_state_dict(convert_state_dict_to_float(checkpoint['wavenet_state_dict']))\n",
    "        gfsq.load_state_dict(convert_state_dict_to_float(checkpoint['gfsq_state_dict']))\n",
    "        decoder.load_state_dict(convert_state_dict_to_float(checkpoint['decoder_state_dict']))\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        logger.info(f\"Resumed training from epoch {start_epoch}\")\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        logger.info(\"No checkpoint found, starting from scratch.\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    \n",
    "\n",
    "# 创建 GradScaler，转换为fp16\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def mel_to_audio(mel_spectrogram, sr=44100, n_fft=1024, hop_length=256, win_length=None):\n",
    "    \"\"\"将 Mel 频谱图转换回音频信号\"\"\"\n",
    "    # 确保输入为 NumPy 数组\n",
    "    if isinstance(mel_spectrogram, torch.Tensor):\n",
    "        mel_spectrogram = mel_spectrogram.cpu().numpy()\n",
    "    \n",
    "    # 使用 librosa 的功能进行逆 Mel 频谱变换\n",
    "    mel_decompress = librosa.feature.inverse.mel_to_audio(mel_spectrogram, sr=sr, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n",
    "    return mel_decompress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charslee/miniconda3/envs/torch/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/charslee/miniconda3/envs/torch/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m     15\u001b[0m     features \u001b[38;5;241m=\u001b[39m wavenet(mel_spectrogram)  \u001b[38;5;66;03m# 通过WaveNet提取特征\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     _, quantized_features, _, _, quantized_indices \u001b[38;5;241m=\u001b[39m \u001b[43mgfsq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 通过GFSQ量化特征\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     decoded_features \u001b[38;5;241m=\u001b[39m decoder(quantized_features)  \u001b[38;5;66;03m# 通过DVAEDecoder解码特征\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# 计算损失\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Repo/public/vq_encoder/modules/dvae.py:103\u001b[0m, in \u001b[0;36mGFSQ.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     99\u001b[0m ind \u001b[38;5;241m=\u001b[39m rearrange(\n\u001b[1;32m    100\u001b[0m     ind, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg b t r ->b t (g r)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    101\u001b[0m )  \n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# 对量化索引进行One-hot编码，这是为了计算每个量化级别的使用频率，进而计算模型的多样性（即复杂度\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m embed_onehot \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_ind\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# 计算One-hot编码的均值，然后用这个均值来计算每个量化级别的使用频率\u001b[39;00m\n\u001b[1;32m    105\u001b[0m e_mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(embed_onehot, dim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练循环\n",
    "num_epochs = 100  # 定义训练的总轮数\n",
    "i = 0\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    wavenet.train()  # 设置WaveNet模型为训练模式\n",
    "    gfsq.train()  # 设置GFSQ模型为训练模式\n",
    "    decoder.train()  # 设置DVAEDecoder模型为训练模式\n",
    "    \n",
    "    for _, mel_spectrogram in enumerate(train_loader):\n",
    "        mel_spectrogram = mel_spectrogram.to(device)  # 将mel谱图数据移动到指定设备（GPU或CPU）\n",
    "        optimizer.zero_grad()  # 清空梯度\n",
    "        \n",
    "        # 前向传播\n",
    "        with autocast():\n",
    "            features = wavenet(mel_spectrogram)  # 通过WaveNet提取特征\n",
    "            _, quantized_features, _, _, quantized_indices = gfsq(features)  # 通过GFSQ量化特征\n",
    "            decoded_features = decoder(quantized_features)  # 通过DVAEDecoder解码特征\n",
    "\n",
    "            # 计算损失\n",
    "            loss = criterion(decoded_features, mel_spectrogram)  # 计算解码后的特征与原始mel谱图之间的均方误差损失\n",
    "            # (loss / accumulation_steps).backward()  # 反向传播并进行梯度累积\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            # optimizer.step()  # 每 accumulation_steps 步更新一次模型参数\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # 打印每100 steps的信息\n",
    "        if (i + 1) % 100 == 0:\n",
    "            logger.info(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {loss.item()}\")\n",
    "            writer.add_scalar('training_loss', loss.item(), epoch * len(train_loader) + i)  # 记录训练损失到TensorBoard\n",
    "\n",
    "        # 每500 steps保存一次模型\n",
    "        if (i + 1) % 500 == 0 or (i+1) == len(train_loader):\n",
    "            checkpoint_path = f'checkpoint_epoch_{epoch+1}_step_{i+1}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'wavenet_state_dict': wavenet.state_dict(),\n",
    "                'gfsq_state_dict': gfsq.state_dict(), \n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),  # 保存 GradScaler 状态\n",
    "            }, checkpoint_path)\n",
    "            logger.info(f\"Model saved to {checkpoint_path}\")\n",
    "        i += 1   # 更新迭代计数器\n",
    "\n",
    "    scheduler.step()  # 每个epoch结束后更新学习率\n",
    "\n",
    "    # 验证模型\n",
    "    wavenet.eval()  # 设置WaveNet模型为评估模式\n",
    "    gfsq.eval()  # 设置GFSQ模型为评估模式\n",
    "    decoder.eval()  # 设置DVAEDecoder模型为评估模式\n",
    "    val_loss_mse = 0  # 初始化验证MSE损失\n",
    "    val_loss_l1 = 0  # 初始化验证L1损失\n",
    "    with torch.no_grad():  # 禁用梯度计算\n",
    "        for batch_index, mel_spectrogram in enumerate(val_loader):\n",
    "            mel_spectrogram = mel_spectrogram.to(device)  # 将mel谱图数据移动到指定设备\n",
    "            with autocast():\n",
    "                features = wavenet(mel_spectrogram)  # 通过WaveNet提取特征\n",
    "                _, quantized_features, _, _, quantized_indices = gfsq(features)  # 通过GFSQ量化特征\n",
    "                decoded_features = decoder(quantized_features)  # 通过DVAEDecoder解码特征\n",
    "                \n",
    "                # 计算MSE损失\n",
    "                loss_mse = F.mse_loss(decoded_features, mel_spectrogram)  # 计算解码后的特征与原始mel谱图之间的均方误差损失\n",
    "                val_loss_mse += loss_mse.item()  # 累加验证MSE损失\n",
    "\n",
    "                # 计算L1损失\n",
    "                loss_l1 = F.l1_loss(decoded_features, mel_spectrogram)  # 计算解码后的特征与原始mel谱图之间的L1损失\n",
    "                val_loss_l1 += loss_l1.item()  # 累加验证L1损失\n",
    "\n",
    "                # 仅在每个 epoch 的第一个 batch 上进行音频解码和可视化\n",
    "                if batch_index == 0:\n",
    "                    # 解码回音频\n",
    "                    mel_spec_np = mel_spectrogram[0].cpu().numpy()\n",
    "                    decoded_features_np = decoded_features[0].cpu().numpy()\n",
    "\n",
    "                    # 调用 librosa 的 mel_to_audio 函数\n",
    "                    audio_original = mel_to_audio(mel_spec_np, n_fft=1024, hop_length=256)\n",
    "                    audio_decoded = mel_to_audio(decoded_features_np, n_fft=1024, hop_length=256)\n",
    "\n",
    "                    # 添加到 TensorBoard\n",
    "                    writer.add_audio('Original/audio', audio_original, global_step=epoch, sample_rate=44100)\n",
    "                    writer.add_audio('Decoded/audio', audio_decoded, global_step=epoch, sample_rate=44100)\n",
    "\n",
    "    val_loss_mse /= len(val_loader)  # 计算平均验证MSE损失\n",
    "    val_loss_l1 /= len(val_loader)  # 计算平均验证L1损失\n",
    "\n",
    "    logger.info(f\"Epoch [{epoch+1}/{num_epochs}], MSE Loss: {val_loss_mse}, L1 Loss: {val_loss_l1}\")\n",
    "    writer.add_scalar('validation_mse_loss', val_loss_mse, epoch)  # 记录验证MSE损失到TensorBoard\n",
    "    writer.add_scalar('validation_l1_loss', val_loss_l1, epoch)  # 记录验证L1损失到TensorBoard\n",
    "\n",
    "logger.info(\"训练完成\")  # 训练完成后打印日志\n",
    "writer.close()  # 关闭TensorBoard日志记录器"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
