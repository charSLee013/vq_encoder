{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from modules.vq_encoder import VQEncoder\n","from modules.dvae import GFSQ, DVAEDecoder\n","import os\n","from torch.utils.tensorboard import SummaryWriter\n","import librosa\n","import torchaudio\n","from torch.utils.data import random_split\n","import logging\n","from torch.cuda.amp import autocast, GradScaler\n","from modules.feature_extractors import MelSpectrogramFeatures"]},{"cell_type":"markdown","metadata":{},"source":["设置设备，优先使用CUDA，其次是MPS（Mac上的GPU加速），最后是CPU"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n","# 设置日志级别为INFO\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger()\n","logger.info(f\"Use device: {device}\")\n","log_dir = \"runs/experiment1\"  # 指定日志目录\n","writer = SummaryWriter(log_dir=log_dir)"]},{"cell_type":"markdown","metadata":{},"source":["初始化模型参数"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_rate = 24000\n","n_fft = 1024\n","hop_length = 256\n","n_mels = 100"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_params = {\n","    \"VQEncoder\": {\"n_mels\": n_mels, \"output_channels\": 1024, 'hop_length': hop_length, 'n_fft': n_fft, 'sample_rate': sample_rate,'residual_channels':1024,},\n","    \"GFSQ\": {\"dim\": 1024, \"levels\": [8, 5, 5, 5], \"G\": 2, \"R\": 2},\n","    \"DVAEDecoder\": {\"idim\": 512, \"odim\": n_mels, \"n_layer\": 12, \"bn_dim\": 128, \"hidden\": 512}\n","}"]},{"cell_type":"markdown","metadata":{},"source":["数据管理"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class AudioDataset(Dataset):\n","    def __init__(self, audio_files, sample_rate=24000,n_fft =1024,hop_length=256,n_mels=100):\n","        # 初始化音频文件列表和Mel谱图转换器\n","        self.audio_files = audio_files\n","        # self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate)\n","        self.mel_spectrogram = MelSpectrogramFeatures(sample_rate=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n","        self.sample_rate = sample_rate\n","    def __len__(self):\n","        # 返回数据集中的音频文件数量\n","        return len(self.audio_files)\n","    # def __getitem__(self, idx):\n","    #     # 加载并返回指定索引的音频文件的Mel谱图\n","    #     mel_spectrogram = self.load_mel_spectrogram(self.audio_files[idx])\n","    #     return mel_spectrogram\n","    def __getitem__(self, idx):\n","        return self.get_item(idx=idx)\n","    # def load_mel_spectrogram(self, file_path):\n","    #     # 加载音频文件并转换为Mel谱图\n","    #     waveform, sr = librosa.load(file_path, sr=self.sample_rate, mono=True)\n","    #     S = librosa.feature.melspectrogram(y=waveform, sr=sr, n_mels=128,n_fft=1024,hop_length=256,)\n","    #     return torch.from_numpy(S)\n","    def load_mel_spectrogram(self, file_path):\n","        # 加载音频文件并转换为Mel谱图\n","        waveform, sr = torchaudio.load(file_path)\n","        if waveform.shape[0] > 1:\n","            waveform = waveform.mean(dim=0)\n","        if sr != self.sample_rate:\n","            waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n","        mel_spectrogram = self.mel_spectrogram(waveform)\n","        return mel_spectrogram[0]\n","    \n","    def get_item(self,idx):\n","        file_path = self.audio_files[idx]\n","        waveform, sr = torchaudio.load(file_path)\n","        if waveform.shape[0] > 1:\n","            waveform = waveform.mean(dim=0)\n","        if sr != self.sample_rate:\n","            waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n","        return waveform.squeeze(0)"]},{"cell_type":"markdown","metadata":{},"source":["过滤掉小于2秒大于30秒的音频"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_audio_files(root_dir):\n","    \"\"\"# 从指定目录加载所有符合条件的音频文件\n","    \"\"\"\n","    audio_files = []\n","    for root, _, files in os.walk(root_dir):\n","        for file in files:\n","            if file.endswith(\".wav\"):\n","                file_path = os.path.join(root, file)\n","                duration = torchaudio.info(file_path).num_frames / torchaudio.info(file_path).sample_rate\n","                if 2 <= duration <= 30:\n","                    audio_files.append(file_path)\n","    return audio_files"]},{"cell_type":"markdown","metadata":{},"source":["定义动态批处理函数"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def dynamic_collate_fn(batch):\n","    batch = [x for x in batch if x is not None]\n","\n","    # Ensure the batch is not empty after filtering\n","    if len(batch) == 0:\n","        raise ValueError(\"All tensors in the batch were skipped. Check your data preprocessing.\")\n","    \n","    # 按照音频长度排序\n","    batch.sort(key=lambda x: len(x), reverse=True)\n","    audio_lengths = torch.tensor([len(x) for x in batch])\n","    max_len = len(batch[0])\n","    \n","    # 填充所有张量到相同的长度\n","    padded_batch = []\n","    for tensor in batch:\n","        padded_tensor = torch.nn.functional.pad(tensor, (0, max_len - len(tensor)), mode='constant', value=0)\n","        padded_batch.append(padded_tensor)\n","    \n","    if len(padded_batch) == 0:\n","        raise ValueError(\"All tensors in the batch were skipped. Check your data preprocessing.\")\n","    \n","    batch_tensor = torch.stack(padded_batch)\n","    return {\n","        \"audios\":batch_tensor,\n","        \"audio_lengths\":audio_lengths,\n","    }"]},{"cell_type":"markdown","metadata":{},"source":["实例化模型"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vqencoder = VQEncoder(**model_params[\"VQEncoder\"]).to(device)\n","gfsq = GFSQ(**model_params[\"GFSQ\"]).to(device)\n","decoder = DVAEDecoder(**model_params[\"DVAEDecoder\"]).to(device)"]},{"cell_type":"markdown","metadata":{},"source":["定义损失函数和优化器"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss_type = 'MSE'\n","if loss_type == 'MSE':\n","    criterion = nn.MSELoss()\n","else:\n","    criterion = nn.L1Loss()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optimizer = optim.Adam(\n","    list(vqencoder.parameters()) + list(gfsq.parameters()) + list(decoder.parameters()), \n","    lr=1e-4,\n","    betas=(0.8, 0.99),\n","    eps=1e-6,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["使用学习率调度器"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import math\n","scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999999)  # 调整调度器参数\n","T_max = 100  # 余弦退火的最大周期为总轮数\n","eta_min = 1e-6  # 最小学习率为1e-6\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)   # 余弦退火\n","# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n","# def get_cosine_schedule_with_warmup_lr_lambda(\n","#     current_step: int,\n","#     num_warmup_steps: int,\n","#     num_training_steps: int,\n","#     num_cycles: float = 0.5,\n","#     final_lr_ratio: float = 0.0,\n","# ):\n","#     if current_step < num_warmup_steps:\n","#         return float(current_step) / float(max(1, num_warmup_steps))\n","#     progress = float(current_step - num_warmup_steps) / float(\n","#         max(1, num_training_steps - num_warmup_steps)\n","#     )\n","#     return max(\n","#         final_lr_ratio,\n","#         0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)),\n","#     )\n","# num_warmup_steps = 100\n","# num_training_steps = 10000\n","# final_lr_ratio = 0\n","# scheduler = torch.optim.lr_scheduler.LambdaLR(\n","#     optimizer, \n","#     lr_lambda=lambda step: get_cosine_schedule_with_warmup_lr_lambda(\n","#         step, \n","#         num_warmup_steps=num_warmup_steps, \n","#         num_training_steps=num_training_steps,\n","#         final_lr_ratio = 0\n","#     )\n","# )"]},{"cell_type":"markdown","metadata":{},"source":["加载数据集并拆分为训练集和验证集"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["root_dir = \"/tmp/three_moon\"\n","audio_files = get_audio_files(root_dir)\n","dataset = AudioDataset(audio_files)"]},{"cell_type":"markdown","metadata":{},"source":["切割分成训练集和校验集"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_size = int(0.95 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","logger.info(f\"Train size: {len(train_dataset)} \\t Val size: {len(val_dataset)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if 'cuda' in str(device):\n","    batch_size = 8\n","else:\n","    batch_size = 1\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=dynamic_collate_fn, )\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=dynamic_collate_fn, )"]},{"cell_type":"markdown","metadata":{},"source":["查找是否有记录点并加载"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import glob  # 用于查找模型文件\n","resume = False  # 如果需要从最新检查点恢复训练，则设置为 True\n","def convert_state_dict_to_float(state_dict):\n","    \"\"\"\n","    将 state_dict 中的所有张量从 fp16 转换为 fp32\n","    \"\"\"\n","    new_state_dict = {}\n","    for k, v in state_dict.items():\n","        new_state_dict[k] = v.float()  # 将每个张量转换为float32\n","    return new_state_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if resume:\n","    checkpoint_files = glob.glob('checkpoint_epoch_*.pth')\n","    if checkpoint_files:\n","        latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n","        checkpoint = torch.load(latest_checkpoint)\n","        vqencoder.load_state_dict(convert_state_dict_to_float(checkpoint['wavenet_state_dict']))\n","        gfsq.load_state_dict(convert_state_dict_to_float(checkpoint['gfsq_state_dict']))\n","        decoder.load_state_dict(convert_state_dict_to_float(checkpoint['decoder_state_dict']))\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        start_epoch = checkpoint['epoch']\n","        logger.info(f\"Resumed training from epoch {start_epoch}\")\n","    else:\n","        start_epoch = 0\n","        logger.info(\"No checkpoint found, starting from scratch.\")\n","else:\n","    start_epoch = 0\n","    "]},{"cell_type":"markdown","metadata":{},"source":["将Mel频谱图转换回音频信号"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import librosa\n","import numpy as np\n","import torch\n","def mel_to_audio(mel_spectrogram, sr=24000, n_fft=1024, hop_length=256, win_length=None):\n","    \"\"\"将 Mel 频谱图转换回音频信号\"\"\"\n","    # 确保输入为 NumPy 数组\n","    if isinstance(mel_spectrogram, torch.Tensor):\n","        mel_spectrogram = mel_spectrogram.cpu().numpy()\n","    \n","    # 使用 librosa 的功能进行逆 Mel 频谱变换\n","    mel_decompress = librosa.feature.inverse.mel_to_audio(mel_spectrogram, sr=sr, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n","    return mel_decompress"]},{"cell_type":"markdown","metadata":{},"source":["定义混合损失函数"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def mixed_loss(decoded_features, mel_spectrogram):\n","    loss_mse = F.mse_loss(decoded_features, mel_spectrogram)\n","    loss_l1 = F.l1_loss(decoded_features, mel_spectrogram)\n","    return loss_mse*0.5 + 0.5 * loss_l1"]},{"cell_type":"markdown","metadata":{},"source":["时间步翻倍"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def double_time_steps(mel_spectrogram):\n","    batch_size, n_mels, time_steps = mel_spectrogram.shape\n","    mel_spectrogram = mel_spectrogram.unsqueeze(1)  # 添加通道维度\n","    doubled_mel = F.interpolate(mel_spectrogram, size=(n_mels, time_steps * 2), mode='bilinear', align_corners=False)\n","    return doubled_mel.squeeze(1)  # 移除通道维度"]},{"cell_type":"markdown","metadata":{},"source":["训练循环"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_epochs = 10000  # 定义训练的总轮数\n","i = 0\n","accumulation_steps = 8 # 梯度累积设置"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scaler = GradScaler() # 创建 GradScaler，转换为fp16\n","for epoch in range(start_epoch, num_epochs):\n","    vqencoder.train()  # 设置WaveNet模型为训练模式\n","    gfsq.train()  # 设置GFSQ模型为训练模式\n","    decoder.train()  # 设置DVAEDecoder模型为训练模式\n","    \n","    for _, batch in enumerate(train_loader):\n","        audios, audio_lengths = batch[\"audios\"], batch[\"audio_lengths\"]\n","        audios = audios.to(device)  # 将mel谱图数据移动到指定设备（GPU或CPU）\n","        optimizer.zero_grad()  # 清空梯度\n","        \n","        # 前向传播\n","        with autocast():\n","            features,origin_mels = vqencoder(audios = audios, audio_lengths = audio_lengths,sr = sample_rate)  # 通过WaveNet提取特征\n","            _, quantized_features, _, _, quantized_indices = gfsq(features)  # 通过GFSQ量化特征\n","\n","            # 将特征沿着 dim=1 维度分成两部分，得到两个形状为 (1, 512, 121) 的张量。\n","            temp = torch.chunk(quantized_features, 2, dim=1) # flatten trick :)\n","            #将这两个张量堆叠在一起，得到形状为 (1, 512, 121, 2) 的张量\n","            temp = torch.stack(temp, -1)\n","            # 重新调整特征形状，得到 vq_feats 形状为 (1, 512, 242)\n","            quantized_features = temp.reshape(*temp.shape[:2], -1)\n","            # 将特征 vq_feats 转置,转置后的特征 vq_feats 形状为 (1, 242, 512)\n","            # quantized_features = vq_feats.transpose(1, 2)\n","            \n","            decoded_mels = decoder(quantized_features)  # 通过DVAEDecoder解码获得Mel频谱图\n","            # 解码的时间步会翻倍，所以也需要将原来的Mel频谱图翻倍处理\n","            origin_mels = double_time_steps(origin_mels)\n","\n","            # 计算损失\n","            # loss = criterion(decoded_mels, origin_mels)  # 计算解码后的特征与原始mel谱图之间的均方误差损失\n","            loss = mixed_loss(decoded_mels, origin_mels)\n","            # (loss / accumulation_steps).backward()  # 反向传播并进行梯度累积\n","        scaler.scale(loss).backward()\n","        \n","        if (i + 1) % accumulation_steps == 0:\n","            # optimizer.step()  # 每 accumulation_steps 步更新一次模型参数\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        # 打印每100 steps的信息\n","        if (i + 1) % 100 == 0:\n","            logger.info(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {loss.item()}\")\n","            writer.add_scalar('training_loss', loss.item(), epoch * len(train_loader) + i)  # 记录训练损失到TensorBoard\n","\n","        # 每500 steps保存一次模型\n","        if (i + 1) % 1000 == 0 or (i+1) == len(train_loader):\n","            checkpoint_path = f'checkpoint_epoch_{epoch+1}_step_{i+1}.pth'\n","            torch.save({\n","                'epoch': epoch,\n","                'wavenet_state_dict': vqencoder.state_dict(),\n","                'gfsq_state_dict': gfsq.state_dict(), \n","                'decoder_state_dict': decoder.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'scaler_state_dict': scaler.state_dict(),  # 保存 GradScaler 状态\n","            }, checkpoint_path)\n","            logger.info(f\"Model saved to {checkpoint_path}\")\n","        i += 1   # 更新迭代计数器\n","    scheduler.step()  # 每个epoch结束后更新学习率\n","\n","    # 验证模型\n","    vqencoder.eval()  # 设置WaveNet模型为评估模式\n","    gfsq.eval()  # 设置GFSQ模型为评估模式\n","    decoder.eval()  # 设置DVAEDecoder模型为评估模式\n","    val_loss_mse = 0  # 初始化验证MSE损失\n","    val_loss_l1 = 0  # 初始化验证L1损失\n","    with torch.no_grad():  # 禁用梯度计算\n","        for batch_index, batch in enumerate(val_loader):\n","            audios, audio_lengths = batch[\"audios\"], batch[\"audio_lengths\"]\n","            audios = audios.to(device)  # 将mel谱图数据移动到指定设备（GPU或CPU）\n","            with autocast():\n","                features,origin_mels = vqencoder(audios = audios, audio_lengths = audio_lengths,sr = sample_rate)  # 通过WaveNet提取特征\n","                _, quantized_features, _, _, quantized_indices = gfsq(features)  # 通过GFSQ量化特征\n","                # 将特征沿着 dim=1 维度分成两部分，得到两个形状为 (1, 512, 121) 的张量。\n","                temp = torch.chunk(quantized_features, 2, dim=1) # flatten trick :)\n","                #将这两个张量堆叠在一起，得到形状为 (1, 512, 121, 2) 的张量\n","                temp = torch.stack(temp, -1)\n","                # 重新调整特征形状，得到 vq_feats 形状为 (1, 512, 242)\n","                quantized_features = temp.reshape(*temp.shape[:2], -1)\n","                # 将特征 vq_feats 转置,转置后的特征 vq_feats 形状为 (1, 242, 512)\n","                # quantized_features = vq_feats.transpose(1, 2)\n","                \n","                decoded_mels = decoder(quantized_features)  # 通过DVAEDecoder解码获得Mel频谱图\n","                # 解码的时间步会翻倍，所以也需要将原来的Mel频谱图翻倍处理\n","                origin_mels = double_time_steps(origin_mels)\n","                \n","                # 计算MSE损失\n","                loss_mse = F.mse_loss(decoded_mels, origin_mels)  # 计算解码后的特征与原始mel谱图之间的均方误差损失\n","                val_loss_mse += loss_mse.item()  # 累加验证MSE损失\n","                # 计算L1损失\n","                loss_l1 = F.l1_loss(decoded_mels, origin_mels)  # 计算解码后的特征与原始mel谱图之间的L1损失\n","                val_loss_l1 += loss_l1.item()  # 累加验证L1损失\n","    val_loss_mse /= len(val_loader)  # 计算平均验证MSE损失\n","    val_loss_l1 /= len(val_loader)  # 计算平均验证L1损失\n","    logger.info(f\"Epoch [{epoch+1}/{num_epochs}], MSE Loss: {val_loss_mse}, L1 Loss: {val_loss_l1}\")\n","    writer.add_scalar('validation_mse_loss', val_loss_mse, epoch)  # 记录验证MSE损失到TensorBoard\n","    writer.add_scalar('validation_l1_loss', val_loss_l1, epoch)  # 记录验证L1损失到TensorBoard"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["logger.info(\"训练完成\")  # 训练完成后打印日志\n","writer.close()  # 关闭TensorBoard日志记录器"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}
