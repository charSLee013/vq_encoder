{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python\n", "# coding: utf-8"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "import torch.nn.functional as F\n", "from torch.utils.data import DataLoader, Dataset\n", "from modules.wavenet import WaveNet\n", "from modules.dvae import GFSQ, DVAEDecoder\n", "from modules.spectrogram import LogMelSpectrogram\n", "import os\n", "import torchaudio\n", "from torch.utils.data import random_split\n", "import logging"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u8bbe\u7f6e\u8bbe\u5907\uff0c\u4f18\u5148\u4f7f\u7528CUDA\uff0c\u5176\u6b21\u662fMPS\uff08Mac\u4e0a\u7684GPU\u52a0\u901f\uff09\uff0c\u6700\u540e\u662fCPU"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n", "# \u8bbe\u7f6e\u65e5\u5fd7\u7ea7\u522b\u4e3aINFO\n", "logging.basicConfig(level=logging.INFO)\n", "logger = logging.getLogger()\n", "logger.info(f\"Use device: {device}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class AudioDataset(Dataset):\n", "    def __init__(self, audio_files, sample_rate=44100):\n", "        # \u521d\u59cb\u5316\u97f3\u9891\u6587\u4ef6\u5217\u8868\u548cMel\u8c31\u56fe\u8f6c\u6362\u5668\n", "        self.audio_files = audio_files\n", "        self.mel_spec = LogMelSpectrogram(\n", "            sample_rate=44100,\n", "            n_fft=2048,\n", "            win_length=2048,\n", "            hop_length=512,\n", "            n_mels=128,\n", "            f_min=0.0,\n", "            f_max=8000.0,\n", "        )\n", "        self.sample_rate = sample_rate\n", "    def __len__(self):\n", "        # \u8fd4\u56de\u6570\u636e\u96c6\u4e2d\u7684\u97f3\u9891\u6587\u4ef6\u6570\u91cf\n", "        return len(self.audio_files)\n", "    def __getitem__(self, idx):\n", "        # \u52a0\u8f7d\u5e76\u8fd4\u56de\u6307\u5b9a\u7d22\u5f15\u7684\u97f3\u9891\u6587\u4ef6\u7684Mel\u8c31\u56fe\n", "        mel_spectrogram = self.load_mel_spectrogram(self.audio_files[idx])\n", "        return mel_spectrogram\n", "    def load_mel_spectrogram(self, file_path):\n", "        # \u52a0\u8f7d\u97f3\u9891\u6587\u4ef6\u5e76\u8f6c\u6362\u4e3aMel\u8c31\u56fe\n", "        waveform, sample_rate = torchaudio.load(file_path)\n", "        if sample_rate != self.sample_rate:\n", "            waveform = torchaudio.transforms.Resample(sample_rate, self.sample_rate)(waveform)\n", "        mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=self.sample_rate)(waveform)\n", "        return mel_spectrogram.squeeze(0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_audio_files(root_dir):\n", "    \"\"\"# \u4ece\u6307\u5b9a\u76ee\u5f55\u52a0\u8f7d\u6240\u6709\u7b26\u5408\u6761\u4ef6\u7684\u97f3\u9891\u6587\u4ef6\n", "    \"\"\"\n", "    audio_files = []\n", "    for root, _, files in os.walk(root_dir):\n", "        for file in files:\n", "            if file.endswith(\".wav\"):\n", "                file_path = os.path.join(root, file)\n", "                duration = torchaudio.info(file_path).num_frames / torchaudio.info(file_path).sample_rate\n", "                if 2 <= duration <= 30:\n", "                    audio_files.append(file_path)\n", "    return audio_files"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def dynamic_collate_fn(batch):\n", "    \"\"\"# \u52a8\u6001\u8c03\u6574\u6279\u5904\u7406\u4e2d\u7684\u97f3\u9891\u957f\u5ea6\uff0c\u786e\u4fdd\u6279\u5904\u7406\u4e2d\u7684\u97f3\u9891\u957f\u5ea6\u76f8\u5bf9\u4e00\u81f4\n", "    \"\"\"\n", "    batch.sort(key=lambda x: x.shape[1], reverse=True)\n", "    max_len = batch[0].shape[1]\n", "    min_len = batch[-1].shape[1]\n", "    \n", "    # \u9009\u62e9\u4e00\u4e2a\u957f\u5ea6\u8303\u56f4\n", "    length_threshold = min_len + (max_len - min_len) // 2\n", "    \n", "    # \u6309\u7167\u957f\u5ea6\u8303\u56f4\u8fdb\u884c\u52a8\u6001\u6279\u5904\u7406\n", "    padded_batch = []\n", "    for tensor in batch:\n", "        if tensor.shape[1] >= length_threshold:\n", "            padded_tensor = torch.nn.functional.pad(tensor, (0, max_len - tensor.shape[1]), mode='constant', value=0)\n", "            padded_batch.append(padded_tensor)\n", "        else:\n", "            break  # \u7ed3\u675f\u5faa\u73af\uff0c\u4e0d\u518d\u6dfb\u52a0\u66f4\u77ed\u7684\u97f3\u9891\n", "    \n", "    if len(padded_batch) == 0:\n", "        raise ValueError(\"All tensors in the batch were skipped. Check your data preprocessing.\")\n", "    batch_tensor = torch.stack(padded_batch)\n", "    return batch_tensor"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u521d\u59cb\u5316\u6a21\u578b\u53c2\u6570"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_params = {\n", "    \"WaveNet\": {\"input_channels\": 128, \"output_channels\": 1024, 'residual_layers': 20, 'dilation_cycle': 4},\n", "    \"GFSQ\": {\"dim\": 1024, \"levels\": [8, 5, 5, 5], \"G\": 2, \"R\": 1},\n", "    \"DVAEDecoder\": {\"idim\": 1024, \"odim\": 128}\n", "}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u5b9e\u4f8b\u5316\u6a21\u578b"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["wavenet = WaveNet(**model_params[\"WaveNet\"]).to(device)\n", "gfsq = GFSQ(**model_params[\"GFSQ\"]).to(device)\n", "decoder = DVAEDecoder(**model_params[\"DVAEDecoder\"]).to(device)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["criterion = nn.MSELoss()\n", "optimizer = optim.Adam(list(wavenet.parameters()) + list(gfsq.parameters()) + list(decoder.parameters()), lr=1e-5)  # \u8c03\u6574\u5b66\u4e60\u7387"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u4f7f\u7528\u5b66\u4e60\u7387\u8c03\u5ea6\u5668"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)  # \u8c03\u6574\u8c03\u5ea6\u5668\u53c2\u6570"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u68af\u5ea6\u7d2f\u79ef\u8bbe\u7f6e"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["accumulation_steps = 8"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u52a0\u8f7d\u6570\u636e\u96c6\u5e76\u62c6\u5206\u4e3a\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["root_dir = \"/tmp/three_moon/\"\n", "audio_files = get_audio_files(root_dir)\n", "dataset = AudioDataset(audio_files)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u5207\u5272\u5206\u6210\u8bad\u7ec3\u96c6\u548c\u6821\u9a8c\u96c6"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_size = int(0.8 * len(dataset))\n", "val_size = len(dataset) - train_size\n", "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=dynamic_collate_fn, )\n", "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=dynamic_collate_fn, )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["logger.info(f\"Train size: {len(train_dataset)} \\t Val size: {len(val_dataset)}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u67e5\u627e\u662f\u5426\u6709\u8bb0\u5f55\u70b9"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import glob  # \u7528\u4e8e\u67e5\u627e\u6a21\u578b\u6587\u4ef6"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u5b9a\u4e49 resume \u53d8\u91cf"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["resume = True  # \u5982\u679c\u9700\u8981\u4ece\u6700\u65b0\u68c0\u67e5\u70b9\u6062\u590d\u8bad\u7ec3\uff0c\u5219\u8bbe\u7f6e\u4e3a True"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u83b7\u53d6\u6700\u65b0\u7684\u68c0\u67e5\u70b9"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if resume:\n", "    checkpoint_files = glob.glob('checkpoint_epoch_*.pth')\n", "    if checkpoint_files:\n", "        latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n", "        checkpoint = torch.load(latest_checkpoint)\n", "        wavenet.load_state_dict(checkpoint['wavenet_state_dict'])\n", "        gfsq.load_state_dict(checkpoint['gfsq_state_dict'])\n", "        decoder.load_state_dict(checkpoint['decoder_state_dict'])\n", "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n", "        start_epoch = checkpoint['epoch']\n", "        logger.info(f\"Resumed training from epoch {start_epoch}\")\n", "    else:\n", "        start_epoch = 0\n", "        logger.info(\"No checkpoint found, starting from scratch.\")\n", "else:\n", "    start_epoch = 0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u8bad\u7ec3\u5faa\u73af"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_epochs = 100  # \u5b9a\u4e49\u8bad\u7ec3\u7684\u603b\u8f6e\u6570\n", "for epoch in range(start_epoch, num_epochs):\n", "    wavenet.train()  # \u8bbe\u7f6eWaveNet\u6a21\u578b\u4e3a\u8bad\u7ec3\u6a21\u5f0f\n", "    gfsq.train()  # \u8bbe\u7f6eGFSQ\u6a21\u578b\u4e3a\u8bad\u7ec3\u6a21\u5f0f\n", "    decoder.train()  # \u8bbe\u7f6eDVAEDecoder\u6a21\u578b\u4e3a\u8bad\u7ec3\u6a21\u5f0f\n", "    \n", "    for i, mel_spectrogram in enumerate(train_loader):\n", "        mel_spectrogram = mel_spectrogram.to(device)  # \u5c06mel\u8c31\u56fe\u6570\u636e\u79fb\u52a8\u5230\u6307\u5b9a\u8bbe\u5907\uff08GPU\u6216CPU\uff09\n", "        optimizer.zero_grad()  # \u6e05\u7a7a\u68af\u5ea6\n", "        \n", "        # \u524d\u5411\u4f20\u64ad\n", "        features = wavenet(mel_spectrogram)  # \u901a\u8fc7WaveNet\u63d0\u53d6\u7279\u5f81\n", "        _, quantized_features, _, _, quantized_indices = gfsq(features)  # \u901a\u8fc7GFSQ\u91cf\u5316\u7279\u5f81\n", "        quantized_features = quantized_features.transpose(1, 2)  # \u8f6c\u7f6e\u91cf\u5316\u7279\u5f81\u4ee5\u9002\u5e94\u89e3\u7801\u5668\u8f93\u5165\n", "        decoded_features = decoder(quantized_features)  # \u901a\u8fc7DVAEDecoder\u89e3\u7801\u7279\u5f81\n", "        decoded_features = decoded_features.transpose(1, 2)  # \u8f6c\u7f6e\u89e3\u7801\u540e\u7684\u7279\u5f81\u4ee5\u5339\u914d\u539f\u59cbmel\u8c31\u56fe\n", "        \n", "        # \u8ba1\u7b97\u635f\u5931\n", "        loss = criterion(decoded_features, mel_spectrogram)  # \u8ba1\u7b97\u89e3\u7801\u540e\u7684\u7279\u5f81\u4e0e\u539f\u59cbmel\u8c31\u56fe\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee\u635f\u5931\n", "        (loss / accumulation_steps).backward()  # \u53cd\u5411\u4f20\u64ad\u5e76\u8fdb\u884c\u68af\u5ea6\u7d2f\u79ef\n", "        \n", "        if (i + 1) % accumulation_steps == 0:\n", "            optimizer.step()  # \u6bcf accumulation_steps \u6b65\u66f4\u65b0\u4e00\u6b21\u6a21\u578b\u53c2\u6570\n\n", "        # \u6253\u5370\u6bcf5 steps\u7684\u4fe1\u606f\n", "        if (i + 1) % 5 == 0:\n", "            logger.info(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {loss.item()}\")\n\n", "        # \u6bcf1000 steps\u4fdd\u5b58\u4e00\u6b21\u6a21\u578b\n", "        if (i + 1) % 1000 == 0:\n", "            checkpoint_path = f'checkpoint_epoch_{epoch+1}_step_{i+1}.pth'\n", "            torch.save({\n", "                'epoch': epoch,\n", "                'wavenet_state_dict': wavenet.state_dict(),\n", "                'gfsq_state_dict': gfsq.state_dict(),\n", "                'decoder_state_dict': decoder.state_dict(),\n", "                'optimizer_state_dict': optimizer.state_dict(),\n", "            }, checkpoint_path)\n", "            logger.info(f\"Model saved to {checkpoint_path}\")\n", "    scheduler.step()  # \u6bcf\u4e2aepoch\u7ed3\u675f\u540e\u66f4\u65b0\u5b66\u4e60\u7387\n\n", "    # \u9a8c\u8bc1\u6a21\u578b\n", "    wavenet.eval()  # \u8bbe\u7f6eWaveNet\u6a21\u578b\u4e3a\u8bc4\u4f30\u6a21\u5f0f\n", "    gfsq.eval()  # \u8bbe\u7f6eGFSQ\u6a21\u578b\u4e3a\u8bc4\u4f30\u6a21\u5f0f\n", "    decoder.eval()  # \u8bbe\u7f6eDVAEDecoder\u6a21\u578b\u4e3a\u8bc4\u4f30\u6a21\u5f0f\n", "    val_loss = 0  # \u521d\u59cb\u5316\u9a8c\u8bc1\u635f\u5931\n", "    with torch.no_grad():  # \u7981\u7528\u68af\u5ea6\u8ba1\u7b97\n", "        for mel_spectrogram in val_loader:\n", "            mel_spectrogram = mel_spectrogram.to(device)  # \u5c06mel\u8c31\u56fe\u6570\u636e\u79fb\u52a8\u5230\u6307\u5b9a\u8bbe\u5907\n", "            features = wavenet(mel_spectrogram)  # \u901a\u8fc7WaveNet\u63d0\u53d6\u7279\u5f81\n", "            _, quantized_features, _, _, quantized_indices = gfsq(features)  # \u901a\u8fc7GFSQ\u91cf\u5316\u7279\u5f81\n", "            quantized_features = quantized_features.transpose(1, 2)  # \u8f6c\u7f6e\u91cf\u5316\u7279\u5f81\u4ee5\u9002\u5e94\u89e3\u7801\u5668\u8f93\u5165\n", "            decoded_features = decoder(quantized_features)  # \u901a\u8fc7DVAEDecoder\u89e3\u7801\u7279\u5f81\n", "            decoded_features = decoded_features.transpose(1, 2)  # \u8f6c\u7f6e\u89e3\u7801\u540e\u7684\u7279\u5f81\u4ee5\u5339\u914d\u539f\u59cbmel\u8c31\u56fe\n", "            loss = criterion(decoded_features, mel_spectrogram)  # \u8ba1\u7b97\u89e3\u7801\u540e\u7684\u7279\u5f81\u4e0e\u539f\u59cbmel\u8c31\u56fe\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee\u635f\u5931\n", "            val_loss += loss.item()  # \u7d2f\u52a0\u9a8c\u8bc1\u635f\u5931\n", "    val_loss /= len(val_loader)  # \u8ba1\u7b97\u5e73\u5747\u9a8c\u8bc1\u635f\u5931\n", "    logger.info(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Val Loss: {val_loss}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["logger.info(\"\u8bad\u7ec3\u5b8c\u6210\")  # \u8bad\u7ec3\u5b8c\u6210\u540e\u6253\u5370\u65e5\u5fd7"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}